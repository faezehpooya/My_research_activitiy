# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CEOQ9u1era33mz_NN-xc95-rlri16peo
"""

# pip install keras_self_attention
#
# pip install 'h5py==2.10.0' --force-reinstall

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
from joblib import Parallel, delayed
from sklearn.model_selection import train_test_split
import copy
import random
import urllib.request
import io
import gzip
import csv
import numpy as np
import keras
from keras import backend as K
from keras.layers import Dense
from keras.utils.vis_utils import plot_model
import json
from keras.models import Sequential
from keras.models import Model
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU
import tensorflow as tf
from keras.layers import Concatenate , Flatten, AveragePooling1D, TimeDistributed, Bidirectional, GRU, concatenate,CuDNNLSTM, Dropout,Conv2DTranspose,Lambda
from keras.layers import Input, Embedding, LSTM, Dense, Convolution2D, GRU, Reshape,MaxPooling2D,Convolution1D,BatchNormalization, Conv1D, MaxPooling1D
from keras_self_attention import SeqSelfAttention
from keras.optimizers import Adam
from keras import regularizers,Sequential
from keras.regularizers import l2
from keras.callbacks import EarlyStopping ,ModelCheckpoint
import matplotlib.pyplot as plt
import random as rn
import tensorflow as tf

f = open('deep_prime_to_sec_protovec_encoding.json',)
one_mer_protovec = json.load(f)

ehsanEmbed =  []
with open("protVec_100d_3grams.csv") as tsvfile:
    tsvreader = csv.reader(tsvfile, delimiter="\t")
    for line in tsvreader:
        ehsanEmbed.append(line[0].split('\t'))
threemers = [vec[0] for vec in ehsanEmbed]
embeddingMat = [[float(n) for n in vec[1:]] for vec in ehsanEmbed]
threemersidx = {} #generate word to index translation dictionary. Use for kmersdict function arguments.
for i, kmer in enumerate(threemers):
    threemersidx[kmer] = i

protvec=dict()
for i in range(len(ehsanEmbed)):
    kmer,encod_kmer=ehsanEmbed[i][0],ehsanEmbed[i][1:]
    protvec[kmer]=encod_kmer
kmer_code= pd.DataFrame(protvec)

amino_aids=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], "PKa":[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}

def read_data(filename,X_index):
    raw_primer= []
    raw_sekunder = []
    idx=0
    j=0
    if filename=='RS126.data.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_sekunder.append(line.strip())
                else:
                    raw_primer.append(line.strip())
                    
    if filename=='pdb_full_train.txt' or filename=='pdb_full_test.txt' or filename=='train.txt' or filename=='blind.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_primer.append(line.strip())

                    
                        
    if filename=='pdb_full_train_dssp.txt' or filename=='pdb_full_test_dssp.txt' or filename=='train_label.txt' or filename=='blind_label.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_sekunder.append(line.strip())

    if filename=='cb513.npy':
        f=np.load(filename, allow_pickle=True , encoding='latin1')
        raw_sekunder=f.item(0)["dssp"]
        raw_primer=f.item(0)["seq"]
        
    if filename=='casp12.npy' or filename=='casp13.npy' or filename=='cullpdb_test.npy' or filename=='cullpdb_train.npy':
        data=np.load(filename).item()
        name=data['name']
        seq=data['seq']
        pssm=data['pssm']
        dssp=data['dssp']
        hhm=data['hhm']
    
    if filename=='cb513.csv' or filename=='cb6133.csv' or filename=='cb6133filtered.csv':
      df=pd.read_csv(filename, sep=',')
      seqs=df[["input"]].to_numpy()
      labels=df[["expected"]].to_numpy()
      raw_primer,raw_seconder=[],[]
      for i in range(len(seqs)):
        raw_primer.append(seqs[i][0])
        raw_seconder.append(labels[i][0])
             
                    
                        
    if filename=='RS126.data.txt' or filename=='cb513.npy':
        return raw_primer,raw_sekunder
    if filename=='casp12.npy' or filename=='casp13.npy' or filename=='cullpdb_test.npy' or filename=='cullpdb_train.npy':
        return name,seq,pssm,dssp,hhm
    if filename=='pdb_full_train.txt' or filename=='pdb_full_test.txt' or filename=='train.txt' or filename=='blind.txt':
        return raw_primer
    if  filename=='pdb_full_train_dssp.txt' or filename=='pdb_full_test_dssp.txt' or filename=='train_label.txt'  or filename=='blind_label.txt':
        return raw_sekunder
    if filename=='cb513.csv' or filename=='cb6133.csv' or filename=='cb6133filtered.csv':
      return raw_primer, raw_seconder
        
        
def preprocess(filename,train,test,max_len,flag):
  if filename=='RS126.txt':
      print("im in if")
      for i in range(len(test)):
          word=str(test[i])
          test[i]=word.replace("_", "C")
          word=str(test[i])
          test[i]=word.replace("-", "C")
          word=str(test[i])
          test[i]=word.replace("G", "C")
          word=str(test[i])
          test[i]=word.replace("I", "C")
          word=str(test[i])
          test[i]=word.replace("B", "C")
          word=str(test[i])
          test[i]=word.replace("T", "C")
          word=str(test[i])
          test[i]=word.replace("S", "C")
          
      for i in range(len(train)):
          word=str(train[i])
          w=str(test[i])
          l=[]
          l=[pos for pos, char in enumerate(word) if char == 'X']
          ll=[pos for pos, char in enumerate(word) if char == 'Z']
          lll=[pos for pos, char in enumerate(w) if char == '?']
          lst=l+ll+lll
          s=train[i]
          t=test[i]
          n=0
          if not flag:
            for j in lst:
                s= s[0 : j-n : ] + s[j-n + 1 : :]
                t= t[0 : j-n : ] + t[j-n + 1 : :]
                n+=1
          train[i]=s
          test[i]=t
      

  raw_primer_train,raw_seconder_train,max_len=equal_length(train,test,max_len)
  return raw_primer_train,raw_seconder_train,max_len




# check if primer and second lens are equal
def equal_length(raw_primer,raw_seconder,max_len):
    count_sekunder = 0
    count_primer = 0
    l=[]
    for i in range(len(raw_seconder)):
        len1 = len(raw_seconder[i])
        len2 = len(raw_primer[i])
        count_sekunder = count_sekunder + len1
        count_primer = count_primer + len2
        if(len1 != len2):
            count_primer-=len2
            count_sekunder-=len1
            l.append(i)
    for i in l:
        raw_primer.pop(i)
        raw_seconder.pop(i)

    print("count struktur sekunder : ",count_sekunder,len(raw_primer))
    print("count struktur primer : ",count_primer,len(raw_seconder))

    for i in range(len(raw_primer)):
      if len(raw_primer[i])>max_len:
        max_len=len(raw_primer[i])

    return raw_primer,raw_seconder,max_len

def find_each_seq_length(raw_primer):
  length=[0 for i in range(len(raw_primer))]
  for i in range(len(raw_primer)):
    length[i]=len(raw_primer[i])
  return length
    


def make_equal_length(raw_primer,raw_seconder,max_len):
    new_primer,new_seconder=['' for i in range(len(raw_primer))],['' for i in range(len(raw_primer))]
    for i in range(len(raw_primer)):
      new_primer[i]=raw_primer[i]
      new_seconder[i]=raw_seconder[i]
      if len(raw_primer[i])<max_len:
        new_primer[i]+=(max_len-len(raw_primer[i]))*'*'
        new_seconder[i]+=(max_len-len(raw_primer[i]))*'*'
    
    for i in range(len(new_primer)):
      if len(new_primer[i])!=max_len:
        print("EERRRROR")
    return new_primer,new_seconder

           
    
def binaryToDecimal(n): 
    return int(n,2) 

def encode_kmer_seconder(label):
    # dssp={'*':0,'L':1,'B':2,'E':3,'G':4,'I':5,'H':6,'S':7,'T':8}
    dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
    lst=[]
    for i in range(len(label)):
      one_hot=[0 for i in range(8)]
      if label[i]  in list(dssp.keys()):
        one_hot[dssp[label[i]]]=1
      lst.append(one_hot)
    return lst


def encode_kmer(seq):
    encoded=[]
    column=list(kmer_code.columns)
    for i in range(len(seq)):
        flag=False
        if seq[i] in column:
            flag=True
            encoded.append(kmer_code[seq[i]].tolist())
            encoded.append(kmer_code[seq[i]].tolist())
            encoded.append(kmer_code[seq[i]].tolist())

        else:
          counter = seq[i].count('*')
          if counter ==1 or counter==2 or counter ==3:
              flag=True
              encoded.append([0 for i in range(100)])
              encoded.append([0 for i in range(100)])
              encoded.append([0 for i in range(100)])
            
    return encoded

def label_to_one_hot(label,max_len):
  # dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7 ,'*':8}
  dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
  out_label=[]
  for i in range(len(label)):
    one_hot=np.zeros((max_len,8))
    for j in range(len(label[i])):
      if label[i][j] in list(dssp.keys()):
        one_hot.itemset((j,dssp[label[i][j]]),1)
    out_label.append(one_hot)
  return np.array(out_label)


def label_to_one_hot2(label):
  # dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7 ,'*':8}
  dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
  out_label=[]
  for i in range(len(label)):
    one_hot=[]
    for j in range(len(label[i])):
      if label[i][j] in list(dssp.keys()):
        l=[0 for i in range(8)]
        l[dssp[label[i][j]]]=1
        one_hot.append(l)
    out_label.append(one_hot)
  return out_label

def seq_arg_max(trainhot):
    train_hot = np.ones((trainhot.shape[0], trainhot.shape[1]))
    for i in range(trainhot.shape[0]):
        for j in range(trainhot.shape[1]):
            if np.sum(trainhot[i,j,:]) != 0:
                train_hot[i,j] = np.argmax(trainhot[i,j,:])
    return train_hot


def seq_to_one_hot(seq,max_len):

  # residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,21))
    for j in range(len(seq[i])):
      if seq[i][j] in list(residue_list.keys()):
        one_hot.itemset((j,residue_list[seq[i][j]]),1)
    out.append(one_hot)

  return np.array(out)

def seq_to_one_hot2(seq):

  # residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
  out=[]
  for i in range(len(seq)):
    one_hot=[]
    for j in range(len(seq[i])):
      if seq[i][j] in list(residue_list.keys()):
        l=[0 for i in range(21)]
        l[residue_list[seq[i][j]]]=1
        one_hot.append(l)
    out.append(one_hot)
  return out


def physiochemical_properties_encoding(seq,max_len):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,9))
    # one_hot=[[0 for i in range(9)] for j in range(max_len)]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        for k in range(len(list(physiochemical_properties.keys()))):
            val=physiochemical_properties[list(physiochemical_properties.keys())[k]][amino_acids_order.index(seq[i][j])]
            one_hot.itemset((j,k),val)
            # one_hot[j][k]=val
      elif seq[i][j]=='X':
        for k in range(9):
          one_hot.itemset((j,k),1)
          # one_hot[j][k]=1      
    out.append(one_hot)
    

  return np.array(out)

def normal_physiochemical_properties_encoding(seq,max_len):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  all_min, all_max={},{}
  normal_physiochemical_properties={}
  for k, v in physiochemical_properties.items():
    min_v, max_v=min(v),max(v)
    all_min[k]=min_v
    all_max[k]=max_v
    normal_physiochemical_properties[k]=[0 for i in range(len(v))]
    for i in range(len(v)):
      # (x – min) / (max – min)
      normal_physiochemical_properties[k][i]=(v[i]-min_v)/(max_v-min_v)
  

  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,9))
    # one_hot=[[0 for i in range(9)] for j in range(max_len)]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        lst_key=list(normal_physiochemical_properties.keys())
        for k in range(len(lst_key)):
            val=normal_physiochemical_properties[list(lst_key)[k]][amino_acids_order.index(seq[i][j])]
            one_hot.itemset((j,k),val)
            # one_hot[j][k]=val
      elif seq[i][j]=='X':
        for k in range(9):
          one_hot.itemset((j,k),1)
          # one_hot[j][k]=1      
    out.append(one_hot)
    

  return np.array(out)



def normal_physiochemical_properties_encoding2(seq):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  all_min, all_max={},{}
  normal_physiochemical_properties={}
  for k, v in physiochemical_properties.items():
    min_v, max_v=min(v),max(v)
    all_min[k]=min_v
    all_max[k]=max_v
    normal_physiochemical_properties[k]=[0 for i in range(len(v))]
    for i in range(len(v)):
      # (x – min) / (max – min)
      normal_physiochemical_properties[k][i]=(v[i]-min_v)/(max_v-min_v)
    
  out=[]
  for i in range(len(seq)):
    one_hot=[[0 for m in range(9)] for o in range(len(seq[i]))]
    # one_hot=[[0 for i in range(9)] for j in range(max_len)]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        lst_key=list(normal_physiochemical_properties.keys())
        for k in range(len(lst_key)):
            val=normal_physiochemical_properties[list(lst_key)[k]][amino_acids_order.index(seq[i][j])]
            # l=[0 for m in range(9)]
            # l[k]=val
            # one_hot.append(l)
            one_hot[j][k]=val
      elif seq[i][j]=='X':
        for k in range(9):
          # one_hot.itemset((j,k),1)
          one_hot[j][k]=1      
    out.append(one_hot)
    

  return out



def seq_to_one_hot_physiochemical_properties(seq,max_len,property):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  out=[]
  lst=physiochemical_properties[property]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,21))
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        one_hot.itemset((j,residue_list[seq[i][j]]),lst[amino_acids_order.index(seq[i][j])])
      if seq[i][j]=='X':
        one_hot.itemset((j,residue_list[seq[i][j]]),1)       
    out.append(one_hot)

  return np.array(out)

def seq_to_one_hot_physiochemical_properties2(seq,property):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  out=[]
  lst=physiochemical_properties[property]
  for i in range(len(seq)):
    one_hot=[]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        l=[0 for m in range(21)]
        l[residue_list[seq[i][j]]]=lst[amino_acids_order.index(seq[i][j])]
        one_hot.append(l)
      if seq[i][j]=='X':
        l=[0 for m in range(21)]
        l[residue_list[seq[i][j]]]=1
        one_hot.append(l)     
    out.append(one_hot)

  return out

def new_encoding_fgene(raw_primer,length):
  order_of_aa_bases_on_BLOSUM=['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H','I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']
  hydropathy_index={'R':-2.5, 'D':-0.9,'N':-0.78,'H':0.4 ,'T':-0.05 ,'Y':0.26 , 'G':0.48 , 
                    'M':0.64, 'L':1.1, 'F':1.2,  'X':0 , 'K':-1.5 , 'Q':-0.85 , 'E':-0.74 , 
                    'S':-0.18 , 'P':0.12 , 'C':0.29 , 'A':0.62 , 'W':0.81 , 'V':1.1 , 'I':1.4 }
  five_class_of_aa_based_of_polarity={0:['X'], 1:['A', 'G', 'I', 'L', 'F', 'P', 'V'], 2:['N', 'C', 'Q', 'S', 'T', 'W', 'Y', 'M'], 3:['D', 'E'], 4:['R', 'H', 'K']}
  X,Y=[],[]
  for i in range(len(raw_primer)):
    seq=raw_primer[i]
    lst=[]
    for j in range(len(seq)):
      aa_encode=[]
      aa=seq[j]
      if aa=='*':
        aa='X'
      idx=order_of_aa_bases_on_BLOSUM.index(aa)
      hydropathy=hydropathy_index[aa]
      l=length[i]
      for k, v in five_class_of_aa_based_of_polarity.items():
        if aa in v:
          class_polarity=k
          break
      aa_encode.append(idx)
      aa_encode.append(hydropathy)
      aa_encode.append(class_polarity)
      aa_encode.append(l)
      lst.append(aa_encode)
    X.append(lst)
  X=np.array(X)
  return X




def split(sequence): 
    return [char for char in sequence]  


def splition(raw_sekunder):
    split_sekunder = []
    for i in range(len(raw_sekunder)):
        split_sekunder.append(split(raw_sekunder[i]))
    return split_sekunder



def kmerlists(seqs,labels):
    all_kmer=[]
    all_labels=[]
    for j in range(len(seqs)):
        kmer0 = []
        label0=[]
        for i in range(0,len(seqs[j])-2,3):
            if len(seqs[j][i:i+3]) == 3:
                kmer0.append(seqs[j][i:i+3])
                label0.append(labels[j][i:i+3])
        all_kmer.append(kmer0)
        all_labels.append(label0)
    return all_kmer,all_labels



def target(lis):
    Y = []
    for i in range(len(lis)):
        for j  in range(len(lis[i])):
            Y.append(lis[i][j])
    return Y

def targett(lis):
  X=[]
  for i in range(len(lis)):
    for j in range(len(lis[i])):
      for k in range(len(lis[i][j])):
        X.append(lis[i][j][k])
  return X


def get_dataset_reshaped(one_hot_X_test,one_hot_Y_test,one_hot_X_valid,one_hot_Y_valid,one_hot_X_train,one_hot_Y_train):
    simple_one_hot_X_train = reshape_data(one_hot_X_train)
    simple_one_hot_X_test = reshape_data(one_hot_X_test)
    simple_one_hot_X_valid = reshape_data(one_hot_X_valid)

    simple_one_hot_Y_train = resphape_labels(one_hot_Y_train)
    simple_one_hot_Y_test = resphape_labels(one_hot_Y_test)
    simple_one_hot_Y_validation = resphape_labels(one_hot_Y_valid)

    return simple_one_hot_X_train, simple_one_hot_X_valid, simple_one_hot_X_test, simple_one_hot_Y_train, simple_one_hot_Y_validation, simple_one_hot_Y_test

def reshape_data(X):
    cnn_width=17
    sequence_len = 702
    amino_acid_residues = 21
    num_classes = 8
    padding = np.zeros((X.shape[0], X.shape[2], int(cnn_width/2)))
    X = np.dstack((padding, np.swapaxes(X, 1, 2), padding))
    X = np.swapaxes(X, 1, 2)
    res = np.zeros((X.shape[0], X.shape[1] - cnn_width + 1, cnn_width, amino_acid_residues))
    for i in range(X.shape[1] - cnn_width + 1):
        res[:, i, :, :] = X[:, i:i+cnn_width, :]
    res = np.reshape(res, (X.shape[0]*(X.shape[1] - cnn_width + 1), cnn_width, amino_acid_residues))
    res = res[np.count_nonzero(res, axis=(1,2))>=(cnn_width*amino_acid_residues-((int(cnn_width/2)*amino_acid_residues)+9*20)), :, :]
    # print("final res is:",res.shape)
    return res

def resphape_labels(labels):
    Y = np.reshape(labels, (labels.shape[0]*labels.shape[1], labels.shape[2]))
    Y = Y[~np.all(Y == 0, axis=1)]
    return Y


def window_padding_data(size, sequence, encode_size):
    num = int(size/2)
    zeros = np.array([0.0 for i in range(encode_size)])
    for i in range(len(sequence)):
        for j in range(num):
                sequence[i].append(zeros)
                sequence[i].insert(0, zeros)
            
    X = []
    temp = []

    for k in range(len(sequence)):
        for l in range(len(sequence[k])-(size-1)):
            temp = sequence[k][l:l+size]
            X.append(temp)
            temp = []

    return X

def check_kmer_exist(x,y,kmer_code):
  col=list(kmer_code.columns)
  new_x,new_y=[[] for i in range(len(x))],[[] for i in range(len(y))]
  for i in range(len(x)):
    for j in range(len(x[i])):
      if x[i][j] in col:
        new_x[i].append(x[i][j])
        new_y[i].append(y[i][j])
      else:
          counter = x[i][j].count('*')
          if counter ==1 or counter==2 or counter ==3:
              new_x[i].append(x[i][j])
              new_y[i].append(y[i][j])
          else:
            new_x[i].append('***')
            new_y[i].append('***') 
  return new_x,new_y

def all_same_length(x,y,max_len):
  for i in range(len(x)):
    for j in range(len(x[i])):
      if len(x[i])!=len(y[i]) or len(x[i])!=max_len or len(y[i])!=max_len:
        print("i is not same len", i)
        print("ERRORR")

def uniq_char(l):
  uniq=[]
  for i in range(len(l)):
    for j in l[i]:
      if j not in uniq:
        uniq.append(j)
  print(uniq)

def protovec_encoding_with_svm_prediction(raw_primer,raw_seconder,max_len,kmer_code):
    raw_primer_train,raw_seconder_train=raw_primer,raw_seconder
    
    split_raw_primer_train,split_raw_seconder_train=kmerlists(raw_primer_train,raw_seconder_train)
    split_raw_primer_train,split_raw_seconder_train=check_kmer_exist(split_raw_primer_train,split_raw_seconder_train,kmer_code)
    
    

    print("after kmerlists")
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(split_raw_primer_train)):
        total_l_primer+=len(split_raw_primer_train[i])
        total_l_seconder+=len(split_raw_seconder_train[i])
    print("total_l_primer is :",total_l_primer)
    print("total_l_seconder is :",total_l_seconder)
    avg_len=total_l_primer//len(split_raw_primer_train)
    
    split_sekunderr=copy.deepcopy(split_raw_seconder_train)
    split_sekunder=[]

    for i in range(len(split_sekunderr)):  
        seq = split_sekunderr[i]
        l=[]
        for j in range(len(seq)):
            lst = encode_kmer_seconder(seq[j])
            for k in range(len(lst)):
                l.append(lst[k])
        split_sekunder.append(l)

    split_primer=copy.deepcopy(split_raw_primer_train)
    for i in range(len(split_primer)):  
        seq2 = split_primer[i]
        encd= list(encode_kmer(seq2))
        for j in range(len(encd)):
          for k in range(len(encd[j])):
            encd[j][k]=float(encd[j][k])
        split_primer[i]=encd
        
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(split_primer)):
        total_l_primer+=len(split_primer[i])
        total_l_seconder+=len(split_sekunder[i])


    all_same_length(split_primer,split_sekunder,max_len)    
      
    X=np.array(split_primer)
    X=X.astype(np.float)
    Y=np.array(split_sekunder)
    return X,Y

def one_mer_protovec_encoding(raw_primer,raw_seconder,max_len,one_mer_protovec):
    raw_primer_train,raw_seconder_train=raw_primer,raw_seconder
    
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(raw_primer_train)):
        total_l_primer+=len(raw_primer_train[i])
        total_l_seconder+=len(raw_seconder_train[i])
    avg_len=total_l_primer//len(raw_primer_train)
    
    split_primer=[]

    for i in range(len(raw_primer_train)):  
        seq = raw_primer_train[i]
        l=[]
        for j in range(len(seq)):
            lst = one_mer_protovec[seq[j]]
            l.append(lst)
        split_primer.append(l)

    split_sekunder=label_to_one_hot(raw_seconder_train,max_len)
    
        
        
    print("after split and encode")
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(split_primer)):
        total_l_primer+=len(split_primer[i])
        total_l_seconder+=len(split_sekunder[i])


    all_same_length(split_primer,split_sekunder,max_len)    
      

    X=np.array(split_primer)
    X=X.astype(np.float)
    Y=np.array(split_sekunder)
    return X,Y

def one_mer_protovec_encoding_window(raw_primer,raw_seconder,max_len,one_mer_protovec):
    raw_primer_train,raw_seconder_train=raw_primer,raw_seconder
    
    size=15

    total_l_primer=0
    total_l_seconder=0
    for i in range(len(raw_primer_train)):
        total_l_primer+=len(raw_primer_train[i])
        total_l_seconder+=len(raw_seconder_train[i])
    avg_len=total_l_primer//len(raw_primer_train)
    
    split_primer=[]

    for i in range(len(raw_primer_train)):  
        seq = raw_primer_train[i]
        l=[]
        for j in range(len(seq)):
            lst = one_mer_protovec[seq[j]]
            l.append(lst)
        split_primer.append(l)

    split_sekunder=label_to_one_hot2(raw_seconder_train)
    
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(split_primer)):
        total_l_primer+=len(split_primer[i])
        total_l_seconder+=len(split_sekunder[i])

    X=window_padding_data(size, split_primer, 50)
    return np.array(X),np.array(Y)

def one_hot_encoding_window(raw_primer,raw_seconder,max_len):
  size=15
  primer,label=seq_to_one_hot2(raw_primer),label_to_one_hot2(raw_seconder)
  X=window_padding_data(size, primer,21)
  Y=target(label)
  return np.array(X),np.array(Y)

def normal_physio_chem_window(raw_primer,raw_seconder):
  size=15
  primer,label=normal_physiochemical_properties_encoding2(raw_primer),label_to_one_hot2(raw_seconder)
  X=window_padding_data(size, primer,9)
  Y=target(label)
  return np.array(X),np.array(Y)

max_len=0
raw_primer_trainn,raw_seconder_trainn = read_data('cb513.csv',None)
raw_primer_test1,raw_seconder_test1,max_len=preprocess('cb513.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)
raw_primer_test,raw_seconder_test=raw_primer_test1,raw_seconder_test1


raw_primer_trainn,raw_seconder_trainn = read_data('cb6133.csv',None)
raw_primer_valid1,raw_seconder_valid1,max_len=preprocess('cb6133.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)
raw_primer_valid,raw_seconder_valid=raw_primer_valid1,raw_seconder_valid1


raw_primer_trainn,raw_seconder_trainn = read_data('cb6133filtered.csv',None)
raw_primer_train,raw_seconder_train,max_len=preprocess('cb6133filtered.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)
max_len+=(3-max_len%3)
length_test, length_valid,length_train=[len(i) for i in raw_primer_test],[len(i) for i in raw_primer_valid],[len(i) for i in raw_primer_train]

raw_primer_test,raw_seconder_test=make_equal_length(raw_primer_test,raw_seconder_test,max_len)
raw_primer_valid,raw_seconder_valid=make_equal_length(raw_primer_valid,raw_seconder_valid,max_len)
raw_primer_train,raw_seconder_train=make_equal_length(raw_primer_train,raw_seconder_train,max_len)

train_idx=random.sample(range(0,len(raw_primer_train)), k=1500)
test_idx=random.sample(range(0,len(raw_primer_test)), k=500)
valid_idx=random.sample(range(0,len(raw_primer_valid)), k=500)
raw_primer_train,raw_seconder_train=[raw_primer_train[train_idx[i]] for i in range(len(train_idx))],[raw_seconder_train[train_idx[i]] for i in range(len(train_idx))]
raw_primer_valid,raw_seconder_valid=[raw_primer_valid[valid_idx[i]] for i in range(len(valid_idx))],[raw_seconder_valid[valid_idx[i]] for i in range(len(valid_idx))]
raw_primer_test,raw_seconder_test=[raw_primer_test[test_idx[i]] for i in range(len(test_idx))],[raw_seconder_test[test_idx[i]] for i in range(len(test_idx))]

X_test,Y_test=protovec_encoding_with_svm_prediction(raw_primer_test,raw_seconder_test,max_len,kmer_code)
X_valid,Y_valid=protovec_encoding_with_svm_prediction(raw_primer_valid,raw_seconder_valid,max_len,kmer_code)
X_train,Y_train=protovec_encoding_with_svm_prediction(raw_primer_train,raw_seconder_train,max_len,kmer_code)

X_test,Y_test=seq_to_one_hot(raw_primer_test,max_len),label_to_one_hot(raw_seconder_test,max_len)
X_valid,Y_valid=seq_to_one_hot(raw_primer_valid,max_len),label_to_one_hot(raw_seconder_valid,max_len)
X_train,Y_train=seq_to_one_hot(raw_primer_train,max_len),label_to_one_hot(raw_seconder_train,max_len)
# X_test_1=seq_arg_max(X_test_1)
# X_valid_1=seq_arg_max(X_valid_1)
# X_train_1=seq_arg_max(X_train_1)

X_test,Y_test=one_hot_encoding_window(raw_primer_test,raw_seconder_test,0)
X_valid,Y_valid=one_hot_encoding_window(raw_primer_valid,raw_seconder_valid,0)
X_train,Y_train=one_hot_encoding_window(raw_primer_train,raw_seconder_train,0)

X_test1,Y_test1=one_mer_protovec_encoding(raw_primer_test,raw_seconder_test,max_len,one_mer_protovec)
X_valid1,Y_valid1=one_mer_protovec_encoding(raw_primer_valid,raw_seconder_valid,max_len,one_mer_protovec)
X_train1,Y_train1=one_mer_protovec_encoding(raw_primer_train,raw_seconder_train,max_len,one_mer_protovec)

X_test1,Y_test1=normal_physio_chem_window(raw_primer_test,raw_seconder_test)
X_valid1,Y_valid1=normal_physio_chem_window(raw_primer_valid,raw_seconder_valid)
X_train1,Y_train1=normal_physio_chem_window(raw_primer_train,raw_seconder_train)

all_physiochemical_properties=list(physiochemical_properties.keys())
all_X_train,all_Y_train={},{}
all_X_test,all_Y_test={},{}
all_X_valid,all_Y_valid={},{}

for i in range(len(all_physiochemical_properties)):
  X_test,Y_test=seq_to_one_hot_physiochemical_properties(raw_primer_test[:300],max_len,all_physiochemical_properties[i]),label_to_one_hot(raw_seconder_test[:300],max_len)
  X_valid,Y_valid=seq_to_one_hot_physiochemical_properties(raw_primer_valid[:400],max_len,all_physiochemical_properties[i]),label_to_one_hot(raw_seconder_valid[:400],max_len)
  X_train,Y_train=seq_to_one_hot_physiochemical_properties(raw_primer_train[:700],max_len,all_physiochemical_properties[i]),label_to_one_hot(raw_seconder_train[:700],max_len)
  all_X_train[all_physiochemical_properties[i]]=X_train
  all_Y_train[all_physiochemical_properties[i]]=Y_train
  all_X_test[all_physiochemical_properties[i]]=X_test
  all_Y_test[all_physiochemical_properties[i]]=Y_test
  all_X_valid[all_physiochemical_properties[i]]=X_valid
  all_Y_valid[all_physiochemical_properties[i]]=Y_valid

X_test,Y_test=physiochemical_properties_encoding(raw_primer_test,max_len),label_to_one_hot(raw_seconder_test,max_len)
X_valid,Y_valid=physiochemical_properties_encoding(raw_primer_valid,max_len),label_to_one_hot(raw_seconder_valid,max_len)
X_train,Y_train=physiochemical_properties_encoding(raw_primer_train,max_len),label_to_one_hot(raw_seconder_train,max_len)

X_test1,Y_test1=normal_physiochemical_properties_encoding(raw_primer_test,max_len),label_to_one_hot(raw_seconder_test,max_len)
X_valid1,Y_valid1=normal_physiochemical_properties_encoding(raw_primer_valid,max_len),label_to_one_hot(raw_seconder_valid,max_len)
X_train1,Y_train1=normal_physiochemical_properties_encoding(raw_primer_train,max_len),label_to_one_hot(raw_seconder_train,max_len)

X_test,Y_test=new_encoding_fgene(raw_primer_test[:300],length_test),label_to_one_hot(raw_seconder_test[:300],max_len)
X_valid,Y_valid=new_encoding_fgene(raw_primer_valid[:400],length_valid),label_to_one_hot(raw_seconder_valid[:400],max_len)
X_train,Y_train=new_encoding_fgene(raw_primer_train[:700],length_train),label_to_one_hot(raw_seconder_train[:700],max_len)

X_train.shape,Y_train.shape, X_valid.shape,Y_valid.shape,X_test.shape, Y_test.shape

X_train1.shape,Y_train1.shape, X_valid1.shape,Y_valid1.shape,X_test1.shape, Y_test1.shape

np.random.seed(1)
rn.seed(1)

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

tf.compat.v1.set_random_seed(0)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
K.set_session(sess)


def model_4(max_len1,encode_size1,encode_size2):
    XDinput = Input(shape=(max_len1, encode_size1),name='main_input1')
    XTinput = Input(shape=(max_len1, encode_size2), name='main_input2')
    l2value=0.001
    NUM_FILTERS=5
    rnn_hidden_size1 = 128
    rnn_hidden_size2 = 128*2
    rnn_hidden_size3 = 128*3
    
    encode_smiles = Convolution1D(NUM_FILTERS,3,activation='relu', border_mode='same', W_regularizer=l2(0.001))(XDinput)
    encode_smiles = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_smiles)
    encode_smiles = Convolution1D(NUM_FILTERS*2,3,activation='relu', border_mode='same', W_regularizer=l2(0.001))(encode_smiles)
    encode_smiles = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_smiles)
    encode_smiles = Convolution1D(NUM_FILTERS*3,3,activation='relu', border_mode='same', W_regularizer=l2(0.001))(encode_smiles)
    encode_smiles = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_smiles) 
    encode_smiles = Dropout(0.4)(encode_smiles)
   
    encode_protein = Bidirectional(LSTM(rnn_hidden_size1, return_sequences=True,inner_activation='relu',dropout_W=0.5,dropout_U=0.5))(XTinput)
    encode_protein = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_protein)
    encode_protein =  Bidirectional(LSTM(rnn_hidden_size2, return_sequences=True,inner_activation='relu',dropout_W=0.5,dropout_U=0.5))(encode_protein)
    encode_protein = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_protein)
    encode_protein =  Bidirectional(LSTM(rnn_hidden_size3, return_sequences=True,inner_activation='relu',dropout_W=0.5,dropout_U=0.5))(encode_protein)
    encode_protein = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_protein) 
    encode_protein = Dropout(0.4)(encode_protein)
    encode_interaction = Concatenate(axis=-1)([encode_protein, encode_smiles])
    encode_interaction = Dropout(0.4)(encode_interaction)
    encode_interaction = Flatten()(encode_interaction)

    

    FC1 = Dense(1024, activation='relu')(encode_interaction)
    FC2 = Dropout(0.4)(FC1)
    FC2 = Dense(512, activation='relu')(FC2)
    FC2 = Dropout(0.4)(FC2)
    FC2 = Dense(128, activation='relu')(FC2)
    predictions = Dense(8, activation='softmax', name='main_output')(FC2)

    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])
    adam = Adam(learning_rate=0.003)
    interactionModel.compile(optimizer=adam,  loss={'main_output': 'categorical_crossentropy'}, weighted_metrics=['accuracy']) 
    print(interactionModel.summary())

    return interactionModel



def deep_aclstm_with_one_input_model(max_len, dim):
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input') 
    concat = main_input
    
    conv1_features = Convolution1D(100,1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(concat)
    conv1_features = Reshape((max_len, 100, 1))(conv1_features)
    
    conv2_features = Convolution2D(100,kernel_size=3,strides=1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv1_features)
    
    conv2_features = Reshape((max_len,100*100))(conv2_features)
    conv2_features = Dropout(0.5)(conv2_features)
    conv2_features = Dense(400, activation='relu')(conv2_features)

    lstm_f1 = LSTM(output_dim=300,return_sequences=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(conv2_features)
    lstm_b1 = LSTM(output_dim=300, return_sequences=True, go_backwards=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(conv2_features)
    lstm_f2 = LSTM(output_dim=300, return_sequences=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(lstm_f1)
    lstm_b2 = LSTM(output_dim=300, return_sequences=True, go_backwards=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(lstm_b1)
    concat_features = Concatenate(axis=-1)([lstm_f2, lstm_b2,conv2_features])

    protein_features=concat_features
    concat_features = Dropout(0.4)(protein_features)
    protein_features = Dense(600,activation='relu')(protein_features)
    dnse=Dense(300,activation='relu')
    protein_features = TimeDistributed(dnse)(protein_features)
    dnse=Dense(50,activation='relu', kernel_regularizer=l2(0.001))
    protein_features = TimeDistributed(dnse)(protein_features)
    
    main_output = Dense(8, activation='softmax', name='main_output')(protein_features)
  
    deepaclstm = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=0.003)
    deepaclstm.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'},  weighted_metrics=['accuracy'])
    deepaclstm.summary()  
    return deepaclstm

def orginal_deep_ACLSTM(max_len,dim1, dim2):
  # design the deepaclstm model
    main_input = Input(shape=(max_len,), dtype='float32', name='main_input1')
    x = Embedding(output_dim=dim2, input_dim=dim2, input_length=max_len)(main_input)
    auxiliary_input = Input(shape=(max_len,dim1), name='main_input2') 
    concat = Concatenate(axis=-1)([x, auxiliary_input])    

    conv1_features = Convolution1D(42,1,activation='relu', border_mode='same', W_regularizer=l2(0.001))(concat)
    conv1_features = Reshape((max_len, 42, 1))(conv1_features)    

    conv2_features = Convolution2D(42,3,1,activation='relu', border_mode='same', W_regularizer=l2(0.001))(conv1_features)
    conv2_features = Reshape((max_len,42*42))(conv2_features)
    conv2_features = Dropout(0.5)(conv2_features)
    conv2_features = Dense(400, activation='relu')(conv2_features)

    lstm_f1 = LSTM(output_dim=300,return_sequences=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(conv2_features)
    lstm_b1 = LSTM(output_dim=300, return_sequences=True, go_backwards=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(conv2_features)    

    lstm_f2 = LSTM(output_dim=300, return_sequences=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(lstm_f1)
    lstm_b2 = LSTM(output_dim=300, return_sequences=True, go_backwards=True,inner_activation='sigmoid',dropout_W=0.5,dropout_U=0.5)(lstm_b1)    
    concat_features= Concatenate(axis=-1)([lstm_f2, lstm_b2, conv2_features])   

    concat_features = Dropout(0.4)(concat_features)
    protein_features = Dense(600,activation='relu')(concat_features)
    dnse=Dense(300,activation='relu')
    protein_features = TimeDistributed(dnse)(protein_features)
    dnse=Dense(50,activation='relu', kernel_regularizer=l2(0.001))
    protein_features = TimeDistributed(dnse)(protein_features)    
    main_output = Dense(8, activation='softmax', name='main_output')(protein_features)  
    

    model = Model(input=[main_input, auxiliary_input], output=[main_output])
    adam = Adam(lr=0.003)
    model.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'}, metrics=['accuracy'], weighted_metrics=['accuracy'])
    model.summary()
    return model

def model_3(max_len,dim):
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
    conv_hidden_size = 64
    rnn_hidden_size = 256
    conv1_features = Convolution1D(conv_hidden_size,3,border_mode='same',activation='relu')(main_input)
    conv2_features = Convolution1D(conv_hidden_size,7,border_mode='same', activation='relu')(main_input)
    conv3_features = Convolution1D(conv_hidden_size,11,border_mode='same',activation='relu')(main_input)
    concat_features= Concatenate(axis=-1)([conv1_features, conv2_features, conv3_features])
    ggru=Bidirectional(LSTM(rnn_hidden_size, return_sequences=True,inner_activation='relu',dropout_W=0.5,dropout_U=0.5))(concat_features)
    concat_features= Concatenate(axis=-1)([ggru, concat_features])
    concat_features = Flatten()(concat_features)
    dnse=Dense(500, activation='relu')(concat_features)
    concat_features=Dropout(0.4)(concat_features)
    dnse=Dense(300, activation='relu')(concat_features)
    concat_features=Dropout(0.4)(concat_features)
    dnse=Dense(50, activation='relu')(concat_features)
    main_output=Dense(8, activation='softmax' , name='main_output')(dnse)
    model = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=0.003)
    model.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'}, weighted_metrics=['accuracy'])
    model.summary()
    return model




def build_model_e(max_len, dim, convs=[3, 5, 7], dense_size=200, dropout_rate=0.5,
                features_to_use=['onehot', 'pssm'], filter_size=256, lr=0.001,
                use_CRF=False):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
            idx = str(idx + 1)
            conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
                Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                      kernel_regularizer=l2(0.001))(main_input)))
    conc = concatenate(conclayers)

    drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
    dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)


    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)
    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='dense'))(dense_convinpn)
        crf = ChainCRF(name="crf1")
        crf_output = crf(timedist)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = TimeDistributed(Dense(8, activation='softmax'), name="main_output")(dense_convinpn)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                      sample_weight_mode='temporal')

    print(model.summary())
    return model
  

def build_model_d(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, features_to_use=['onehot', 'pssm'], filter_size=256, lr=0.001, use_CRF=False, attention_units=32, attention_type='additive'):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
            idx = str(idx + 1)
            conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
                Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                      kernel_regularizer=l2(0.001))(main_input)))
    conc = concatenate(conclayers)

    if dropout_rate > 0:
          drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)
    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    lstm_out = Dense(dense_size, activation='relu')(drop_after_lstm)


    # Attention layer
    seq_representation = SeqSelfAttention(units=attention_units, attention_type=attention_type,
                                          name='Attention')(lstm_out)
    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='timedist'))(seq_representation)
        crf = ChainCRF(name="crf1")
        main_output = crf(timedist)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=0.001)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = TimeDistributed(Dense(8, activation='softmax'), name="main_output")(seq_representation)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=0.001)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                      sample_weight_mode='temporal')
    print(model.summary())
    return model




def build_model_c(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, CRF_input_dim=200, lr=0.001):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
            idx = str(idx + 1)
            conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
                Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                      kernel_regularizer=l2(0.001))(main_input)))
    conc = concatenate(conclayers)

    if dropout_rate > 0:
          drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)
    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(CRF_input_dim, activation='relu')(drop_after_lstm)

    timedist = TimeDistributed(Dense(8, name='crf_in'))(dense_out)
    crf = ChainCRF(name="crf1")
    main_output = crf(timedist)
    model = Model(input=[main_input], output=[main_output])
    adam=optimizers.Adam(lr=lr)
    model.compile(loss=crf.loss, optimizer=adam, weighted_metrics= ['accuracy'], sample_weight_mode='temporal')
    print(model.summary())
    return model

def build_model_b(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001, use_CRF=False):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)

    # Highway
    highway_layer = Dense(dense_size, activation='relu')(drop_after_lstm)
    highway_out = Dense(dense_size, activation='relu')(highway_layer)

    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='crf_in'))(highway_out)
        crf = ChainCRF(name="crf1")
        main_output = crf(timedist, name="main_output")
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = TimeDistributed(Dense(8, activation='softmax'), name="main_output")(highway_out)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                      sample_weight_mode='temporal')
    print(model.summary())
    return model

def build_model_a(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001):
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]
    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(dense_size, activation='relu')(drop_after_lstm)

    # Labeling layer layer
    main_output = TimeDistributed(Dense(8, activation='softmax'), name='main_output')(dense_out)
    # model = Model(inputs=main_input, outputs=timedist)
    model = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=lr)
    model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                  sample_weight_mode='temporal')

    print(model.summary())
    return model

def build_model_a_window(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001):
    l2value=0.0001
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]
    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=regularizers.l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(dense_size, activation='relu')(drop_after_lstm)
    

    main_output= Flatten()(dense_out)
    main_output = Dense(1000,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(500,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(100,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(20,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(8,activation='relu',kernel_regularizer=l2(l2value), name='main_output')(main_output)
    model = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=lr)
    model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'])

    print(model.summary())
    return model


def main(train_1,train_2,train_label,val_1,val_2,val_label,test_1,test_2,test_label,max_len,dim1, dim2,str):

    # model = deep_aclstm_with_one_input_model(max_len,dim1)
    # model = orginal_deep_ACLSTM(max_len,dim1,dim2)
    # model = new_model(max_len, dim1)
    # model = build_model_e(max_len=max_len, dim=dim1)
    model = build_model_d(max_len=max_len, dim=dim1)
    # model = build_model_c(max_len=max_len, dim=dim1)
    # model = build_model_b(max_len=max_len, dim=dim1)
    # model = build_model_a_window(max_len=max_len, dim=dim1)
    # model = new_model_1(max_len,dim1,dim2)

    earlyStopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, mode='auto')
    load_file = "ac_LSTM_best_time_"+str+".h5" # M: val_loss E: val_weighted_accuracy
    checkpointer = ModelCheckpoint(filepath=load_file,verbose=1,save_best_only=True)

    history=model.fit({'main_input': train_1}, {'main_output': train_label}, validation_data=({'main_input': val_1},{'main_output': val_label}),
            epochs=35, batch_size=42,callbacks=[checkpointer], verbose=1, shuffle=True)

    # history=model.fit({'main_input1': train_1, 'main_input2': train_2}, {'main_output': train_label},validation_data=({'main_input1': val_1, 'main_input2': val_2},{'main_output': val_label}),
    #         nb_epoch=45, batch_size=64, callbacks=[checkpointer], verbose=1, shuffle=True)
     

    model.load_weights(load_file)
    print("#########evaluate:##############")
    score = model.evaluate({'main_input': test_1},{'main_output': test_label}, verbose=1, batch_size=8)
    # score = model.evaluate({'main_input1': test_1, 'main_input2': test_2},{'main_output': test_label}, verbose=1, batch_size=8)
    print(score) 
    print('test loss:', score[0])
    print('test accuracy:', score[1])

model=model_4(700,21,9)
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

main(X_train1,None,Y_train1,X_valid1,None,Y_valid1,X_test1,None,Y_test1,max_len,9,0,"nbuild_model_d_normalphsio_without_window1")


