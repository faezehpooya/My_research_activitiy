# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dzrsfcFjulNw3mth_RfOZli8iBAn_BsT
"""

# pip install keras_self_attention
#
# pip install 'h5py==2.10.0' --force-reinstall
#
# pip install numpy==1.19.5

import tensorflow as tf
import numpy as np 
sys_details = tf.sysconfig.get_build_info()
cudnn_version = sys_details["cudnn_version"]
cuda_version = sys_details["cuda_version"]
cpu_compiler = sys_details["cpu_compiler"]

print(cpu_compiler)
print(tf.__version__)
print(cuda_version)
print(cudnn_version)
print(np.__version__)
# print(keras.__version__)

import sys
import pandas as pd
import numpy as np
import copy
import random
import urllib.request
import csv
import numpy as np
import json

from tensorflow.keras.applications import (
        vgg16,
        resnet50,
        mobilenet,
        inception_v3)
from keras.applications.vgg16 import VGG16

from tensorflow import keras
from tensorflow.keras.layers import Embedding, Conv1D, Conv2D, MaxPooling1D, Dropout, Conv2DTranspose, Lambda, Concatenate , Flatten,Dense, Input,BatchNormalization,Reshape
from tensorflow.keras import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.regularizers import l2
from keras.optimizers import Adam

import tensorflow.keras.backend as K

import tensorflow as tf

def num_to_hot(X):
  train_X=[]
  for i in range(len(X)):
    protein=X[i]
    add_p=[]
    for aa in protein:
      hot=[0 for i in range(21)]
      if aa!=0:
        hot[aa-1]=1
      else:
        hot[20]=1
      add_p.append(hot)
    train_X.append(add_p)
  return np.array(train_X)

def from1Dto2D(arg_in):
    b_size = tf.shape(arg_in)[0] # getting batch size
    dim=16

    
    ## matrix multiplications are performed separately for each band.
    # for band 0
    b0_1 = tf.linalg.matmul(arg_in[:,:,0:1], tf.ones((b_size,1,dim)))
    b0_2 = tf.linalg.matmul(tf.ones((b_size,dim,1)), arg_in[:,:,0:1], transpose_b=True)

    # for band 1
    b1_1 = tf.linalg.matmul(arg_in[:,:,1:2], tf.ones((b_size,1,dim)))
    b1_2 = tf.linalg.matmul(tf.ones((b_size,dim,1)), arg_in[:,:,1:2], transpose_b=True)

    # for band 2
    b2_1 = tf.linalg.matmul(arg_in[:,:,2:3], tf.ones((b_size,1,dim)))
    b2_2 = tf.linalg.matmul(tf.ones((b_size,dim,1)), arg_in[:,:,2:3], transpose_b=True)

    # for band 3
    b3_1 = tf.linalg.matmul(arg_in[:,:,3:4], tf.ones((b_size,1,dim)))
    b3_2 = tf.linalg.matmul(tf.ones((b_size,dim,1)), arg_in[:,:,3:4], transpose_b=True)
  
    
    return tf.stack((b0_1, b1_1, b2_1, b3_1, b0_2, b1_2, b2_2, b3_2), axis=-1)

def my_pretrained_model(new_model):
  layers = [l for l in vgg_model.layers]
  main_input = Input(shape=(128,21),dtype='float32', name='main_input')
  conv1_features = Conv1D(300,1,activation='relu', padding='same', kernel_regularizer=l2(0.001), name='my_conv1d')(main_input)
  conv1_features = keras.layers.BatchNormalization()(conv1_features)
  conv1_features = Reshape((128, 300, 1))(conv1_features)    
  new_conv = Conv2D(3,3,activation='relu', padding='same', kernel_regularizer=l2(0.001), name='my_conv2d')(conv1_features)
  new_conv = keras.layers.BatchNormalization()(new_conv)
  x = new_conv
  for i in range(1, len(layers)):
    # layers[i].trainable = False
    x = layers[i](x)
    if i==1 or i==2 or i==4 or i==5 or i==7 or i==8 or i==9 or i==11 or i==12 or i==13 or i==15 or i==16 or i==17:
      x = keras.layers.BatchNormalization()(x)
    if i==18:
      break

  flat = Flatten()(x)
  print(flat.shape)
  flat = Reshape((16,-1))(flat)
  flat = Lambda(from1Dto2D)(flat)

  '''Section 4: decoder (last part of the network) with 2D operations '''

  conv_tr = Conv2DTranspose(64, (1, 1), activation='relu', padding='same')(flat)
  conv_tr = Conv2DTranspose(64, (1, 1), activation='relu', padding='same')(conv_tr)

  conv_tr = Conv2DTranspose(128, (3, 3), activation='relu',  padding='same')(conv_tr)
  conv_tr = Dropout(0.5)(conv_tr)
  conv_tr = Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='relu',  padding='same')(conv_tr)
  conv_tr = Conv2DTranspose(128, (3, 3), activation='relu',  padding='same')(conv_tr)
  conv_tr = Dropout(0.5)(conv_tr)
  conv_tr = Conv2DTranspose(128, (3, 3), strides=(2, 2), activation='relu',  padding='same')(conv_tr)
  conv_tr = Conv2DTranspose(64, (3, 3), activation='relu',  padding='same')(conv_tr)
  conv_tr = Dropout(0.5)(conv_tr)
  conv_tr = Conv2DTranspose(64, (3, 3), strides=(2, 2), activation='relu',  padding='same')(conv_tr)
  conv_tr = Conv2DTranspose(64, (3, 3), activation='relu',  padding='same')(conv_tr)
  conv_tr = Dropout(0.5)(conv_tr)
  conv_tr = Conv2DTranspose(32, (3, 3), activation='relu',  padding='same')(conv_tr)

  conv_tr = Conv2DTranspose(16, (1, 1), activation='relu', padding='same')(conv_tr)
  conv_tr = Conv2DTranspose(8, (1, 1), activation='relu', padding='same')(conv_tr)
  predictions = Conv2DTranspose(1, (1, 1), activation='relu', padding='same')(conv_tr)



  # FC1 = Dense(1024, activation='relu')(flat)
  # FC2 = Dropout(0.4)(FC1)
  # FC2 = Dense(512, activation='relu')(FC2)
  # FC2 = Dropout(0.4)(FC2)
  # FC2 = Dense(128, activation='relu')(FC2)
  # predictions = Dense(8, activation='softmax', name='main_output')(FC2) 
  new_model2 = keras.Model(inputs=main_input, outputs=predictions)
  adam = Adam(lr=0.003)
  new_model2.compile(loss='mean_squared_error', optimizer=adam) # setting loss and optimizer
  # new_model2.summary()

  return new_model2

def my_pretrained_model_2(new_model, max_len, dim):
  layers = [l for l in new_model.layers]
  main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
  new_conv = Conv1D(21,1,activation='relu', padding='same', kernel_regularizer=l2(0.001), name='my_conv11d')(main_input)
  x = new_conv
  for i in range(1, len(layers)):
    if i==3:
      x= Reshape((max_len, 300, 1))(x)
    else:  
      layers[i].trainable = False
      x = layers[i](x)
  
  flat = Flatten()(x)
  reshpe = Reshape((128,-1))(flat)  #added
  conv1_features = Conv1D(max_len,1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(reshpe) # added
  flat = Flatten()(conv1_features) # added
  flat = Reshape((max_len,-1))(flat)  #added
  FC1 = Dense(1024, activation='relu')(flat)
  FC2 = Dropout(0.4)(FC1)
  FC2 = Dense(512, activation='relu')(FC2)
  FC2 = Dropout(0.4)(FC2)
  FC2 = Dense(128, activation='relu')(FC2)
  predictions = Dense(8, activation='softmax', name='main_output')(FC2)
  new_model2 = keras.Model(inputs=main_input, outputs=predictions)
  
  adam = Adam(lr=0.003)
  new_model2.compile(optimizer = adam, loss='categorical_crossentropy',  weighted_metrics=['accuracy'])
  # new_model2.summary()

  return new_model2

txtfile = open('my_output.txt', 'w')

cc=b = np.load('amino_acid_sequences.npz')
dd=np.load('C_alpha_atom_locations.npz')
x_train = cc['x_train']
x_test = cc['x_test']
y_train = dd['y_train']
y_test = dd['y_test']

train_X,test_X=num_to_hot(x_train), num_to_hot(x_test)

printstr='***********************************\n'+ 'phase 1 predict third structure \n'+ "x_train.shape,x_test.shape, train_X.shape,test_X.shape is: " + str(x_train.shape) + str(x_test.shape) + str(train_X.shape) + str(test_X.shape) + "\n"
txtfile.write(printstr)

cmap_train = np.zeros((y_train.shape[0], y_train.shape[1], y_train.shape[1], 1))

for i1 in range(y_train.shape[0]):    
    for i2 in range(y_train.shape[1]):
        
        # if amino acid sequence ends, loop continues saving computing time.
        if x_train[i1, i2] == 0:
            continue
        
        for i3 in range(y_train.shape[1]):
            
            # if amino acid sequence ends, loop continues saving computing time.
            if x_train[i1, i3] == 0:
                continue
            
            # distance calculation and thresholding
            d_sqr = (y_train[i1,i2,0]-y_train[i1,i3,0])**2 + (y_train[i1,i2,1]-y_train[i1,i3,1])**2 + (y_train[i1,i2,2]-y_train[i1,i3,2])**2
            cmap_train[i1, i2, i3, 0] = (d_sqr**0.5) > 900 # 9 angstrom distance threshold (900 threshold value)

cmap_test = np.zeros((y_test.shape[0], y_test.shape[1], y_test.shape[1], 1))

for i1 in range(y_test.shape[0]):
    for i2 in range(y_test.shape[1]):
        
        # if amino acid sequence ends, loop continues saving computing time.
        if x_test[i1, i2] == 0:
            continue
        
        for i3 in range(y_test.shape[1]):
            
            # if amino acid sequence ends, loop continues saving computing time.
            if x_test[i1, i3] == 0:
                continue
            
            # distance calculation and thresholding
            d_sqr = (y_test[i1,i2,0]-y_test[i1,i3,0])**2 + (y_test[i1,i2,1]-y_test[i1,i3,1])**2 + (y_test[i1,i2,2]-y_test[i1,i3,2])**2
            cmap_test[i1, i2, i3, 0] = (d_sqr**0.5) > 900 # 9 angstrom distance threshold (900 threshold value)

printstr="train_X.shape,test_X.shape,cmap_train.shape, cmap_test.shape is: " + str(train_X.shape) + str(test_X.shape) + str(cmap_train.shape) + str(cmap_test.shape) + "\n"
txtfile.write(printstr)

org_vgg_model = VGG16(weights='imagenet')

org_vgg_model.summary()

from keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
import cv2, numpy as np

def my_vgg16(weights_path=None):
    model = Sequential()
    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(64, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(128, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(256, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Convolution2D(512, 3, 3, activation='relu'))
    model.add(MaxPooling2D((2,2), strides=(2,2)))
    
    if weights_path:
        model.load_weights(weights_path)

    return model

# vgg_model = VGG16(weights='imagenet') 
vgg_model = VGG16(weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False)

vgg_model.summary()

model_1=my_pretrained_model(vgg_model)

with open('model_1.txt','w') as fh:
    model_1.summary(print_fn=lambda x: fh.write(x + '\n'))

load_file = "primary_to_contact_map_vgg16_imagenet.h5" 
cp_callback = ModelCheckpoint(filepath=load_file,save_best_only=True, verbose=1)
fit_h = model_1.fit(train_X, cmap_train, batch_size=256, epochs=50, verbose=1, shuffle=1, validation_data=(test_X,cmap_test), callbacks=[cp_callback])

with open('model_1_history.json', 'w') as f:
    json.dump(fit_h.history, f)

model_1.save('saved_model2/my_model')

new_model = tf.keras.models.load_model('saved_model2/my_model', custom_objects={'tf': tf})

def read_data(filename,X_index):
    raw_primer= []
    raw_sekunder = []
    idx=0
    j=0
    if filename=='RS126.data.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_sekunder.append(line.strip())
                else:
                    raw_primer.append(line.strip())
                    
    if filename=='pdb_full_train.txt' or filename=='pdb_full_test.txt' or filename=='train.txt' or filename=='blind.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_primer.append(line.strip())

                    
                        
    if filename=='pdb_full_train_dssp.txt' or filename=='pdb_full_test_dssp.txt' or filename=='train_label.txt' or filename=='blind_label.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_sekunder.append(line.strip())

    if filename=='cb513.npy':
        f=np.load(filename, allow_pickle=True , encoding='latin1')
        raw_sekunder=f.item(0)["dssp"]
        raw_primer=f.item(0)["seq"]
        
    if filename=='casp12.npy' or filename=='casp13.npy' or filename=='cullpdb_test.npy' or filename=='cullpdb_train.npy':
        data=np.load(filename, allow_pickle=True , encoding='latin1').item()
        name=data['name']
        seq=data['seq']
        pssm=data['pssm']
        dssp=data['dssp']
        hhm=data['hhm']
    
    if filename=='cb513.csv' or filename=='cb6133.csv' or filename=='cb6133filtered.csv':
      df=pd.read_csv(filename, sep=',')
      seqs=df[["input"]].to_numpy()
      labels=df[["expected"]].to_numpy()
      raw_primer,raw_seconder=[],[]
      for i in range(len(seqs)):
        raw_primer.append(seqs[i][0])
        raw_seconder.append(labels[i][0])


    if filename=='single_domain_dssp_annotations.json' or filename=='full_protein_dssp_annotations.json':
      f = open(filename,)
      data = json.load(f)
      ID=list(data.keys())
      primer,seconder=[],[]
      for i in range(len(ID)):
        seq= data[ID[i]]['Sequence']
        dssp=data[ID[i]]['DSSP']
        primer.append(seq)
        seconder.append(dssp)
             
                    
                        
    if filename=='RS126.data.txt' or filename=='cb513.npy':
        return raw_primer,raw_sekunder
    if filename=='casp12.npy' or filename=='casp13.npy' or filename=='cullpdb_test.npy' or filename=='cullpdb_train.npy':
        return seq,dssp, name
    if filename=='pdb_full_train.txt' or filename=='pdb_full_test.txt' or filename=='train.txt' or filename=='blind.txt':
        return raw_primer
    if  filename=='pdb_full_train_dssp.txt' or filename=='pdb_full_test_dssp.txt' or filename=='train_label.txt'  or filename=='blind_label.txt':
        return raw_sekunder
    if filename=='cb513.csv' or filename=='cb6133.csv' or filename=='cb6133filtered.csv':
      return raw_primer, raw_seconder
    if filename=='single_domain_dssp_annotations.json' or  filename=='full_protein_dssp_annotations.json':
      return primer, seconder,ID
        
        
def preprocess(filename,train,test,max_len,flag):
  if filename=='RS126.txt':
      print("im in if")
      for i in range(len(test)):
          word=str(test[i])
          test[i]=word.replace("_", "C")
          word=str(test[i])
          test[i]=word.replace("-", "C")
          word=str(test[i])
          test[i]=word.replace("G", "C")
          word=str(test[i])
          test[i]=word.replace("I", "C")
          word=str(test[i])
          test[i]=word.replace("B", "C")
          word=str(test[i])
          test[i]=word.replace("T", "C")
          word=str(test[i])
          test[i]=word.replace("S", "C")
          
      for i in range(len(train)):
          word=str(train[i])
          w=str(test[i])
          l=[]
          l=[pos for pos, char in enumerate(word) if char == 'X']
          ll=[pos for pos, char in enumerate(word) if char == 'Z']
          lll=[pos for pos, char in enumerate(w) if char == '?']
          lst=l+ll+lll
          s=train[i]
          t=test[i]
          n=0
          if not flag:
            for j in lst:
                s= s[0 : j-n : ] + s[j-n + 1 : :]
                t= t[0 : j-n : ] + t[j-n + 1 : :]
                n+=1
          train[i]=s
          test[i]=t
      

  raw_primer_train,raw_seconder_train,max_len=equal_length(train,test,max_len)
  return raw_primer_train,raw_seconder_train,max_len




# check if primer and second lens are equal
def equal_length(raw_primer,raw_seconder,max_len):
    count_sekunder = 0
    count_primer = 0
    l=[]
    for i in range(len(raw_seconder)):
        len1 = len(raw_seconder[i])
        len2 = len(raw_primer[i])
        count_sekunder = count_sekunder + len1
        count_primer = count_primer + len2
        if(len1 != len2):
            count_primer-=len2
            count_sekunder-=len1
            l.append(i)
            # print("im here length primer and seconder weren't equal")
    for i in l:
        raw_primer.pop(i)
        raw_seconder.pop(i)

    # print("count struktur sekunder : ",count_sekunder,len(raw_primer))
    # print("count struktur primer : ",count_primer,len(raw_seconder))

    for i in range(len(raw_primer)):
      if len(raw_primer[i])>max_len:
        max_len=len(raw_primer[i])

    return raw_primer,raw_seconder,max_len

def find_each_seq_length(raw_primer):
  length=[0 for i in range(len(raw_primer))]
  for i in range(len(raw_primer)):
    length[i]=len(raw_primer[i])
  return length
    


def make_equal_length(raw_primer,raw_seconder,max_len):
    new_primer,new_seconder=['' for i in range(len(raw_primer))],['' for i in range(len(raw_primer))]
    for i in range(len(raw_primer)):
      new_primer[i]=raw_primer[i]
      new_seconder[i]=raw_seconder[i]
      if len(raw_primer[i])<max_len:
        new_primer[i]+=(max_len-len(raw_primer[i]))*'*'
        new_seconder[i]+=(max_len-len(raw_primer[i]))*'*'
    
    for i in range(len(new_primer)):
      if len(new_primer[i])!=max_len:
        print("EERRRROR")
    return new_primer,new_seconder

           
    
def binaryToDecimal(n): 
    return int(n,2) 

def encode_kmer_seconder(label):
    # dssp={'*':0,'L':1,'B':2,'E':3,'G':4,'I':5,'H':6,'S':7,'T':8}
    dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
    lst=[]
    for i in range(len(label)):
      one_hot=[0 for i in range(8)]
      if label[i]  in list(dssp.keys()):
        one_hot[dssp[label[i]]]=1
      lst.append(one_hot)
    return lst


def encode_kmer(seq):
    encoded=[]
    column=list(kmer_code.columns)
    for i in range(len(seq)):
        flag=False
        if seq[i] in column:
            flag=True
            encoded.append(kmer_code[seq[i]].tolist())
            encoded.append(kmer_code[seq[i]].tolist())
            encoded.append(kmer_code[seq[i]].tolist())

        else:
          counter = seq[i].count('*')
          if counter ==1 or counter==2 or counter ==3:
              flag=True
              encoded.append([0 for i in range(100)])
              encoded.append([0 for i in range(100)])
              encoded.append([0 for i in range(100)])
        if not flag:
          print("^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^")
          print("seq[i] not found in coulumn")
          print("seq[i] is :",seq[i])
          print("^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^")
            
    return encoded

def label_to_one_hot(label,max_len):
  # dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7 ,'*':8}
  dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
  out_label=[]
  for i in range(len(label)):
    one_hot=np.zeros((max_len,8))
    for j in range(len(label[i])):
      if label[i][j] in list(dssp.keys()):
        one_hot.itemset((j,dssp[label[i][j]]),1)
    out_label.append(one_hot)
  return np.array(out_label)


def label_to_one_hot2(label):
  # dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7 ,'*':8}
  dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
  out_label=[]
  for i in range(len(label)):
    one_hot=[]
    for j in range(len(label[i])):
      if label[i][j] in list(dssp.keys()):
        l=[0 for i in range(8)]
        l[dssp[label[i][j]]]=1
        one_hot.append(l)
    out_label.append(one_hot)
  return out_label

def seq_arg_max(trainhot):
    train_hot = np.ones((trainhot.shape[0], trainhot.shape[1]))
    for i in range(trainhot.shape[0]):
        for j in range(trainhot.shape[1]):
            if np.sum(trainhot[i,j,:]) != 0:
                train_hot[i,j] = np.argmax(trainhot[i,j,:])
    return train_hot


def seq_to_one_hot(seq,max_len):

  # residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,21))
    for j in range(len(seq[i])):
      if seq[i][j] in list(residue_list.keys()):
        one_hot.itemset((j,residue_list[seq[i][j]]),1)
    out.append(one_hot)

  return np.array(out)

def seq_to_one_hot2(seq):

  # residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
  out=[]
  for i in range(len(seq)):
    one_hot=[]
    for j in range(len(seq[i])):
      if seq[i][j] in list(residue_list.keys()):
        l=[0 for i in range(21)]
        l[residue_list[seq[i][j]]]=1
        one_hot.append(l)
    out.append(one_hot)
  return out


def physiochemical_properties_encoding(seq,max_len):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,9))
    # one_hot=[[0 for i in range(9)] for j in range(max_len)]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        for k in range(len(list(physiochemical_properties.keys()))):
            val=physiochemical_properties[list(physiochemical_properties.keys())[k]][amino_acids_order.index(seq[i][j])]
            one_hot.itemset((j,k),val)
            # one_hot[j][k]=val
      elif seq[i][j]=='X':
        for k in range(9):
          one_hot.itemset((j,k),1)
          # one_hot[j][k]=1      
    out.append(one_hot)
    

  return np.array(out)

def normal_physiochemical_properties_encoding(seq,max_len):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  all_min, all_max={},{}
  normal_physiochemical_properties={}
  for k, v in physiochemical_properties.items():
    min_v, max_v=min(v),max(v)
    all_min[k]=min_v
    all_max[k]=max_v
    normal_physiochemical_properties[k]=[0 for i in range(len(v))]
    for i in range(len(v)):
      # (x – min) / (max – min)
      normal_physiochemical_properties[k][i]=(v[i]-min_v)/(max_v-min_v)
    




  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,9))
    # one_hot=[[0 for i in range(9)] for j in range(max_len)]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        lst_key=list(normal_physiochemical_properties.keys())
        for k in range(len(lst_key)):
            val=normal_physiochemical_properties[list(lst_key)[k]][amino_acids_order.index(seq[i][j])]
            one_hot.itemset((j,k),val)
            # one_hot[j][k]=val
      elif seq[i][j]=='X':
        for k in range(9):
          one_hot.itemset((j,k),1)
          # one_hot[j][k]=1      
    out.append(one_hot)
    

  return np.array(out)



def normal_physiochemical_properties_encoding2(seq):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  all_min, all_max={},{}
  normal_physiochemical_properties={}
  for k, v in physiochemical_properties.items():
    min_v, max_v=min(v),max(v)
    all_min[k]=min_v
    all_max[k]=max_v
    normal_physiochemical_properties[k]=[0 for i in range(len(v))]
    for i in range(len(v)):
      # (x – min) / (max – min)
      normal_physiochemical_properties[k][i]=(v[i]-min_v)/(max_v-min_v)
    




  out=[]
  for i in range(len(seq)):
    one_hot=[[0 for m in range(9)] for o in range(len(seq[i]))]
    # one_hot=[[0 for i in range(9)] for j in range(max_len)]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        lst_key=list(normal_physiochemical_properties.keys())
        for k in range(len(lst_key)):
            val=normal_physiochemical_properties[list(lst_key)[k]][amino_acids_order.index(seq[i][j])]
            # l=[0 for m in range(9)]
            # l[k]=val
            # one_hot.append(l)
            one_hot[j][k]=val
      elif seq[i][j]=='X':
        for k in range(9):
          # one_hot.itemset((j,k),1)
          one_hot[j][k]=1      
    out.append(one_hot)
    

  return out



def seq_to_one_hot_physiochemical_properties(seq,max_len,property):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  out=[]
  lst=physiochemical_properties[property]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,21))
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        one_hot.itemset((j,residue_list[seq[i][j]]),lst[amino_acids_order.index(seq[i][j])])
      if seq[i][j]=='X':
        one_hot.itemset((j,residue_list[seq[i][j]]),1)       
    out.append(one_hot)

  return np.array(out)

def seq_to_one_hot_physiochemical_properties2(seq,property):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']
  physiochemical_properties={"Pka(COOH)":[2.34,1.71,2.09,2.19,1.83,2.34,1.82,2.36,2.18,2.36,2.28,2.02,1.99,2.17,2.17,2.21,2.63,2.32,2.38,2.2],
                           "Pka(NH3)" :[9.69,10.78,9.82,9.67,9.13,9.6,9.17,9.68,8.95,9.6,9.21,8.8,10.6,9.13,9.04,9.15,10.43,9.62,9.39,9.11], 
                           "PI"       :[6.02,5.02,2.97,3.22,5.48,5.97,7.59,6.02,9.74,5.98,5.75,5.41,6.3,5.65,10.76,5.68,6.53,5.97,5.89,5.66],
                           "Mw"       :[89.06,121.12,133.6,147.08,165.09,75.05,155.09,131.11,146.13,131.11,149.15,132.6,115.08,146.08,174.4,105.06,119.18,117.09,204.11,181.09],
                           "H2O"      :[1.8,-16.5,5,12,-34.5,0,-38.5,12.4,13.5,-11,-10,-5.3,-86.2,6.3,12.5,-7.5,-28.5,5.6,-33.7,0],
                           "HCL"      :[14.6,6.5,25.4,31.8,-4.5,0,11.8,39.5,26,16,23.2,33.2,-60.4,31.8,27.6,15.1,15,28.3,2.8,-10],
                           "Hy"       :[1.8,2.5,-3.5,-3.5,2.8,-0.4,-3.2,4.5,-3.9,3.8,1.9,-3.5,-1.6,-3.5,-4.5,-0.8,-0.7,4.2,-0.9,-1.3], 
                           "PKa"      :[0,1,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1],
                           "PKa(R)"   :[0,8.33,3.86,4.25,0,0,6,0,10.53,0,0,0,0,0,12.48,0,0,0,0,10.07]}
  out=[]
  lst=physiochemical_properties[property]
  for i in range(len(seq)):
    one_hot=[]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        l=[0 for m in range(21)]
        l[residue_list[seq[i][j]]]=lst[amino_acids_order.index(seq[i][j])]
        one_hot.append(l)
      if seq[i][j]=='X':
        l=[0 for m in range(21)]
        l[residue_list[seq[i][j]]]=1
        one_hot.append(l)     
    out.append(one_hot)

  return out

def new_encoding_fgene(raw_primer,length):
  order_of_aa_bases_on_BLOSUM=['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H','I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']
  hydropathy_index={'R':-2.5, 'D':-0.9,'N':-0.78,'H':0.4 ,'T':-0.05 ,'Y':0.26 , 'G':0.48 , 
                    'M':0.64, 'L':1.1, 'F':1.2,  'X':0 , 'K':-1.5 , 'Q':-0.85 , 'E':-0.74 , 
                    'S':-0.18 , 'P':0.12 , 'C':0.29 , 'A':0.62 , 'W':0.81 , 'V':1.1 , 'I':1.4 }
  five_class_of_aa_based_of_polarity={0:['X'], 1:['A', 'G', 'I', 'L', 'F', 'P', 'V'], 2:['N', 'C', 'Q', 'S', 'T', 'W', 'Y', 'M'], 3:['D', 'E'], 4:['R', 'H', 'K']}
  X,Y=[],[]
  for i in range(len(raw_primer)):
    seq=raw_primer[i]
    lst=[]
    for j in range(len(seq)):
      aa_encode=[]
      aa=seq[j]
      if aa=='*':
        aa='X'
      idx=order_of_aa_bases_on_BLOSUM.index(aa)
      hydropathy=hydropathy_index[aa]
      l=length[i]
      for k, v in five_class_of_aa_based_of_polarity.items():
        if aa in v:
          class_polarity=k
          break
      aa_encode.append(idx)
      aa_encode.append(hydropathy)
      aa_encode.append(class_polarity)
      aa_encode.append(l)
      lst.append(aa_encode)
    X.append(lst)
  X=np.array(X)
  return X




def split(sequence): 
    return [char for char in sequence]  


def splition(raw_sekunder):
    split_sekunder = []
    for i in range(len(raw_sekunder)):
        split_sekunder.append(split(raw_sekunder[i]))
    return split_sekunder



def kmerlists(seqs,labels):
    all_kmer=[]
    all_labels=[]
    for j in range(len(seqs)):
        kmer0 = []
        label0=[]
        for i in range(0,len(seqs[j])-2,3):
            if len(seqs[j][i:i+3]) == 3:
                kmer0.append(seqs[j][i:i+3])
                label0.append(labels[j][i:i+3])
        all_kmer.append(kmer0)
        all_labels.append(label0)
    return all_kmer,all_labels



def target(lis):
    Y = []
    for i in range(len(lis)):
        for j  in range(len(lis[i])):
            Y.append(lis[i][j])
    return Y

def targett(lis):
  X=[]
  for i in range(len(lis)):
    for j in range(len(lis[i])):
      for k in range(len(lis[i][j])):
        X.append(lis[i][j][k])
  return X


def get_dataset_reshaped(one_hot_X_test,one_hot_Y_test,one_hot_X_valid,one_hot_Y_valid,one_hot_X_train,one_hot_Y_train):
    simple_one_hot_X_train = reshape_data(one_hot_X_train)
    simple_one_hot_X_test = reshape_data(one_hot_X_test)
    simple_one_hot_X_valid = reshape_data(one_hot_X_valid)

    simple_one_hot_Y_train = resphape_labels(one_hot_Y_train)
    simple_one_hot_Y_test = resphape_labels(one_hot_Y_test)
    simple_one_hot_Y_validation = resphape_labels(one_hot_Y_valid)

    return simple_one_hot_X_train, simple_one_hot_X_valid, simple_one_hot_X_test, simple_one_hot_Y_train, simple_one_hot_Y_validation, simple_one_hot_Y_test

def reshape_data(X):
    cnn_width=17
    sequence_len = 702
    amino_acid_residues = 21
    num_classes = 8
    padding = np.zeros((X.shape[0], X.shape[2], int(cnn_width/2)))
    X = np.dstack((padding, np.swapaxes(X, 1, 2), padding))
    # print("now x.shape is :", X.shape)
    X = np.swapaxes(X, 1, 2)
    # print("now x.shape is :", X.shape)
    res = np.zeros((X.shape[0], X.shape[1] - cnn_width + 1, cnn_width, amino_acid_residues))
    # print("res.shape is :",res.shape)
    for i in range(X.shape[1] - cnn_width + 1):
        res[:, i, :, :] = X[:, i:i+cnn_width, :]
    res = np.reshape(res, (X.shape[0]*(X.shape[1] - cnn_width + 1), cnn_width, amino_acid_residues))
    # print("now res is :", res.shape)
    # print(np.count_nonzero(res, axis=(1,2))>(int(cnn_width/2)*amino_acid_residues))
    res = res[np.count_nonzero(res, axis=(1,2))>=(cnn_width*amino_acid_residues-((int(cnn_width/2)*amino_acid_residues)+9*20)), :, :]
    # print("final res is:",res.shape)
    return res

def resphape_labels(labels):
    Y = np.reshape(labels, (labels.shape[0]*labels.shape[1], labels.shape[2]))
    Y = Y[~np.all(Y == 0, axis=1)]
    return Y


def window_padding_data(size, sequence, encode_size):
    num = int(size/2)
    zeros = np.array([0.0 for i in range(encode_size)])
    for i in range(len(sequence)):
        for j in range(num):
                sequence[i].append(zeros)
                sequence[i].insert(0, zeros)
            
    X = []
    temp = []

    for k in range(len(sequence)):
        for l in range(len(sequence[k])-(size-1)):
            temp = sequence[k][l:l+size]
            X.append(temp)
            temp = []

    return X

def check_kmer_exist(x,y,kmer_code):
  col=list(kmer_code.columns)
  new_x,new_y=[[] for i in range(len(x))],[[] for i in range(len(y))]
  for i in range(len(x)):
    for j in range(len(x[i])):
      if x[i][j] in col:
        new_x[i].append(x[i][j])
        new_y[i].append(y[i][j])
      else:
          counter = x[i][j].count('*')
          if counter ==1 or counter==2 or counter ==3:
              new_x[i].append(x[i][j])
              new_y[i].append(y[i][j])
          else:
            new_x[i].append('***')
            new_y[i].append('***') 
  return new_x,new_y

def all_same_length(x,y,max_len):
  for i in range(len(x)):
    for j in range(len(x[i])):
      if len(x[i])!=len(y[i]) or len(x[i])!=max_len or len(y[i])!=max_len:
        print("i is not same len", i)
        print("ERRORR")

def uniq_char(l):
  uniq=[]
  for i in range(len(l)):
    for j in l[i]:
      if j not in uniq:
        uniq.append(j)
  print(uniq)

def one_mer_protovec_encoding_window(raw_primer,raw_seconder,max_len,one_mer_protovec):
    raw_primer_train,raw_seconder_train=raw_primer,raw_seconder
    
    size=15

    print("after kmerlists")
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(raw_primer_train)):
        total_l_primer+=len(raw_primer_train[i])
        total_l_seconder+=len(raw_seconder_train[i])
    print("total_l_primer is :",total_l_primer)
    print("total_l_seconder is :",total_l_seconder)
    avg_len=total_l_primer//len(raw_primer_train)
    
    split_primer=[]

    for i in range(len(raw_primer_train)):  
        seq = raw_primer_train[i]
        l=[]
        for j in range(len(seq)):
            lst = one_mer_protovec[seq[j]]
            l.append(lst)
        split_primer.append(l)

    split_sekunder=label_to_one_hot2(raw_seconder_train)
    
        
        
    print("after split and encode")
    total_l_primer=0
    total_l_seconder=0
    for i in range(len(split_primer)):
        total_l_primer+=len(split_primer[i])
        total_l_seconder+=len(split_sekunder[i])
    print("total_l_primer is :",total_l_primer)
    print("total_l_seconder is :",total_l_seconder)

    X=window_padding_data(size, split_primer, 50)
    return np.array(X),np.array(Y)

def one_hot_encoding_window(raw_primer,raw_seconder):
  size=35
  primer,label=seq_to_one_hot2(raw_primer),label_to_one_hot2(raw_seconder)
  X=window_padding_data(size, primer,21)
  Y=target(label)
  return np.array(X),np.array(Y)

def normal_physio_chem_window(raw_primer,raw_seconder):
  size=35
  primer,label=normal_physiochemical_properties_encoding2(raw_primer),label_to_one_hot2(raw_seconder)
  X=window_padding_data(size, primer,9)
  Y=target(label)
  return np.array(X),np.array(Y)

# without padding

max_len=0
raw_primer_trainn,raw_seconder_trainn = read_data('cb513.csv',None)
raw_primer_test3,raw_seconder_test3,max_len=preprocess('cb513.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)


raw_primer_trainn,raw_seconder_trainn = read_data('cb6133filtered.csv',None)
raw_primer_valid,raw_seconder_valid,max_len=preprocess('cb6133filtered.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)


raw_primer_trainn,raw_seconder_trainn,ID_s = read_data('single_domain_dssp_annotations.json',None)
raw_primer_train,raw_seconder_train,max_len=preprocess('single_domain_dssp_annotations.json',raw_primer_trainn,raw_seconder_trainn,max_len,False)


raw_primer_trainn,raw_seconder_trainn,ID_F = read_data('full_protein_dssp_annotations.json',None)
raw_primer_test_1,raw_seconder_test_1,max_len=preprocess('full_protein_dssp_annotations.json',raw_primer_trainn,raw_seconder_trainn,max_len,False)

train_idx=random.sample(range(0,len(raw_primer_train)), k=6000)
test_idx_1=random.sample(range(0,len(raw_primer_test_1)), k=300)
remain_test_idx=[]
for i in range(len(raw_primer_test_1)):
  if i not in test_idx_1:
    remain_test_idx.append(i)

test_idx_2=random.sample(remain_test_idx, k=300)

raw_primer_train,raw_seconder_train=[raw_primer_train[train_idx[i]] for i in range(len(train_idx))],[raw_seconder_train[train_idx[i]] for i in range(len(train_idx))]

raw_primer_test1,raw_seconder_test1=[raw_primer_test_1[test_idx_1[i]] for i in range(len(test_idx_1))],[raw_seconder_test_1[test_idx_1[i]] for i in range(len(test_idx_1))]
raw_primer_test2,raw_seconder_test2=[raw_primer_test_1[test_idx_2[i]] for i in range(len(test_idx_2))],[raw_seconder_test_1[test_idx_2[i]] for i in range(len(test_idx_2))]

printstr='***********************************\n'+ 'phase 2 predict second structure \n'+ "raw_primer_train,raw_seconder_train,raw_primer_valid,raw_seconder_valid,raw_primer_test1,raw_seconder_test1,raw_primer_test2,raw_seconder_test2,raw_primer_test3,raw_seconder_test3  is: " + str(len(raw_primer_train)) + str(len(raw_seconder_train)) + str(len(raw_primer_valid)) + str(len(raw_seconder_valid)) + str(len(raw_primer_test1))+ str(len(raw_seconder_test1)) + str(len(raw_primer_test2)) + str(len(raw_seconder_test2)) + str(len(raw_primer_test3)) + str(len(raw_seconder_test3)) + "\n"
txtfile.write(printstr)

window_phys_X_test1,window_phys_Y_test1=normal_physio_chem_window(raw_primer_test1,raw_seconder_test1)
window_phys_X_test2,window_phys_Y_test2=normal_physio_chem_window(raw_primer_test2,raw_seconder_test2)
window_phys_X_test3,window_phys_Y_test3=normal_physio_chem_window(raw_seconder_test3,raw_primer_test3)
window_phys_X_valid,window_phys_Y_valid=normal_physio_chem_window(raw_primer_valid,raw_seconder_valid)
window_phys_X_train,window_phys_Y_train=normal_physio_chem_window(raw_primer_train,raw_seconder_train)

printstr="window_phys_X_test1.shape, window_phys_Y_test1.shape, window_phys_X_test2.shape, window_phys_Y_test2.shape, window_phys_X_test3.shape, window_phys_Y_test3.shape, window_phys_X_valid.shape, window_phys_Y_valid.shape ,window_phys_X_train.shape, window_phys_Y_train.shape is :" +
str(window_phys_X_test1.shape) + str(window_phys_Y_test1.shape) + str(window_phys_X_test2.shape) + str(window_phys_Y_test2.shape) + str(window_phys_X_test3.shape) +  str(window_phys_Y_test3.shape) + str(window_phys_X_valid.shape) + str(window_phys_Y_valid.shape) + str(window_phys_X_train.shape) + str(window_phys_Y_train.shape)  + "\n"
txtfile.write(printstr)

model_2=my_pretrained_model_2(new_model, 35, 9)

with open('model_2_with_window.txt','w') as fh:
    model_2.summary(print_fn=lambda x: fh.write(x + '\n'))

load_file = "secondary_structure_vgg16_imagenet_pretrained_with_window_normal_phys.h5" # M: val_loss E: val_weighted_accuracy
checkpointer = ModelCheckpoint(filepath=load_file,verbose=1,save_best_only=True)

m2_history=model_2.fit({'main_input': window_phys_X_train}, {'main_output': window_phys_Y_train}, validation_data=({'main_input': window_phys_X_valid},{'main_output': window_phys_Y_valid}),
        epochs=100, batch_size=256,callbacks=[checkpointer], verbose=1, shuffle=True)


with open('model_2_with_window_normal_phys_history.json', 'w') as f:
    json.dump(m2_history.history, f)
  
model_2.load_weights(load_file)
print("#########evaluate:##############")
score = model_2.evaluate({'main_input': window_phys_X_test1},{'main_output': window_phys_Y_test1}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " with window normal phys test1 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_2.load_weights(load_file)
score = model_2.evaluate({'main_input': window_phys_X_test2},{'main_output': window_phys_Y_test2}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])


printstr="#########evaluate:############## \n"+ " with window normal phys test2 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_2.load_weights(load_file)
score = model_2.evaluate({'main_input': window_phys_X_test3},{'main_output': window_phys_Y_test3}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " with window normal phys test3 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

window_hot_X_test1,window_hot_Y_test1=one_hot_encoding_window(raw_primer_test1,raw_seconder_test1)
window_hot_X_test2,window_hot_Y_test2=one_hot_encoding_window(raw_primer_test2,raw_seconder_test2)
window_hot_X_test3,window_hot_Y_test3=one_hot_encoding_window(raw_seconder_test3,raw_primer_test3)
window_hot_X_valid,window_hot_Y_valid=one_hot_encoding_window(raw_primer_valid,raw_seconder_valid)
window_hot_X_train,window_hot_Y_train=one_hot_encoding_window(raw_primer_train,raw_seconder_train)

window_hot_X_test1.shape, window_hot_Y_test1.shape, window_hot_X_test2.shape, window_hot_Y_test2.shape, window_hot_X_test3.shape, window_hot_Y_test3.shape, window_hot_X_valid.shape, window_hot_Y_valid.shape ,window_hot_X_train.shape, window_hot_Y_train.shape
printstr="window_hot_X_test1.shape, window_hot_Y_test1.shape, window_hot_X_test2.shape, window_hot_Y_test2.shape, window_hot_X_test3.shape, window_hot_Y_test3.shape, window_hot_X_valid.shape, window_hot_Y_valid.shape ,window_hot_X_train.shape, window_hot_Y_train.shape is :" + str(window_hot_X_test1.shape) + str(window_hot_Y_test1.shape) + str(window_hot_X_test2.shape) + str(window_hot_Y_test2.shape) + str(window_hot_X_test3.shape) +  str(window_hot_Y_test3.shape) + str(window_hot_X_valid.shape) + str(window_hot_Y_valid.shape) + str(window_hot_X_train.shape) + str(window_hot_Y_train.shape)  + "\n"
txtfile.write(printstr)

model_2=my_pretrained_model_2(new_model, 35, 21)

load_file = "secondary_structure_vgg16_imagenet_pretrained_with_window_one_hot.h5" # M: val_loss E: val_weighted_accuracy
checkpointer = ModelCheckpoint(filepath=load_file,verbose=1,save_best_only=True)

m2_history=model_2.fit({'main_input': window_hot_X_train}, {'main_output': window_hot_Y_train}, validation_data=({'main_input': window_hot_X_valid},{'main_output': window_hot_Y_valid}),
        epochs=100, batch_size=256,callbacks=[checkpointer], verbose=1, shuffle=True)
  
with open('model_2_with_window_hot_history.json', 'w') as f:
    json.dump(m2_history.history, f)


model_2.load_weights(load_file)
print("#########evaluate:##############")
score = model_2.evaluate({'main_input': window_hot_X_test1},{'main_output': window_hot_Y_test1}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " with window one hot test1 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_2.load_weights(load_file)
score = model_2.evaluate({'main_input': window_hot_X_test2},{'main_output': window_hot_Y_test2}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " with window one hot test2 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_2.load_weights(load_file)
score = model_2.evaluate({'main_input': window_hot_X_test3},{'main_output': window_hot_Y_test3}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " with window one hot test3 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

# with padding

max_len=0
raw_primer_trainn,raw_seconder_trainn = read_data('cb513.csv',None)
raw_primer_test3,raw_seconder_test3,max_len=preprocess('cb513.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)


raw_primer_trainn,raw_seconder_trainn = read_data('cb6133filtered.csv',None)
raw_primer_valid,raw_seconder_valid,max_len=preprocess('cb6133filtered.csv',raw_primer_trainn,raw_seconder_trainn,max_len,False)


raw_primer_trainn,raw_seconder_trainn,ID_s = read_data('single_domain_dssp_annotations.json',None)
raw_primer_train,raw_seconder_train,max_len=preprocess('single_domain_dssp_annotations.json',raw_primer_trainn,raw_seconder_trainn,max_len,False)


raw_primer_trainn,raw_seconder_trainn,ID_F = read_data('full_protein_dssp_annotations.json',None)
raw_primer_test_1,raw_seconder_test_1,max_len=preprocess('full_protein_dssp_annotations.json',raw_primer_trainn,raw_seconder_trainn,max_len,False)

train_idx=random.sample(range(0,len(raw_primer_train)), k=10000)
test_idx_1=random.sample(range(0,len(raw_primer_test_1)), k=300)
remain_test_idx=[]
for i in range(len(raw_primer_test_1)):
  if i not in test_idx_1:
    remain_test_idx.append(i)

test_idx_2=random.sample(remain_test_idx, k=300)

raw_primer_train,raw_seconder_train=[raw_primer_train[train_idx[i]] for i in range(len(train_idx))],[raw_seconder_train[train_idx[i]] for i in range(len(train_idx))]

raw_primer_test1,raw_seconder_test1=[raw_primer_test_1[test_idx_1[i]] for i in range(len(test_idx_1))],[raw_seconder_test_1[test_idx_1[i]] for i in range(len(test_idx_1))]
raw_primer_test2,raw_seconder_test2=[raw_primer_test_1[test_idx_2[i]] for i in range(len(test_idx_2))],[raw_seconder_test_1[test_idx_2[i]] for i in range(len(test_idx_2))]

max_len=0
for i in raw_primer_train:
  if len(i)>max_len:
    max_len= len(i)
for i in raw_primer_valid:
  if len(i)>max_len:
    max_len=len(i)
for i in raw_primer_test1:
  if len(i)>max_len:
    max_len=len(i)
for i in raw_primer_test2:
  if len(i)>max_len:
    max_len=len(i)
for i in raw_primer_test3:
  if len(i)>max_len:
    max_len=len(i)

raw_primer_test1,raw_seconder_test1=make_equal_length(raw_primer_test1,raw_seconder_test1,max_len)
raw_primer_test2,raw_seconder_test2=make_equal_length(raw_primer_test2,raw_seconder_test2,max_len)
raw_primer_test3,raw_seconder_test3=make_equal_length(raw_primer_test3,raw_seconder_test3,max_len)
raw_primer_valid,raw_seconder_valid=make_equal_length(raw_primer_valid,raw_seconder_valid,max_len)
raw_primer_train,raw_seconder_train=make_equal_length(raw_primer_train,raw_seconder_train,max_len)

phys_X_test1,phys_Y_test1=normal_physiochemical_properties_encoding(raw_primer_test1,max_len),label_to_one_hot(raw_seconder_test1,max_len)
phys_X_test2,phys_Y_test2=normal_physiochemical_properties_encoding(raw_primer_test2,max_len),label_to_one_hot(raw_seconder_test2,max_len)
phys_X_test3,phys_Y_test3=normal_physiochemical_properties_encoding(raw_primer_test3,max_len),label_to_one_hot(raw_seconder_test3,max_len)
phys_X_valid,phys_Y_valid=normal_physiochemical_properties_encoding(raw_primer_valid,max_len),label_to_one_hot(raw_seconder_valid,max_len)
phys_X_train,phys_Y_train=normal_physiochemical_properties_encoding(raw_primer_train,max_len),label_to_one_hot(raw_seconder_train,max_len)

printstr="phys_X_test1.shape, phys_Y_test1.shape, phys_X_test2.shape, phys_Y_test2.shape, phys_X_test3.shape, phys_Y_test3.shape, phys_X_valid.shape, phys_Y_valid.shape ,phys_X_train.shape, phys_Y_train.shape is :" + str(phys_X_test1.shape) + str(phys_Y_test1.shape) + str(phys_X_test2.shape) + str(phys_Y_test2.shape) + str(phys_X_test3.shape) +  str(phys_Y_test3.shape) + str(phys_X_valid.shape) + str(phys_Y_valid.shape) + str(phys_X_train.shape) + str(phys_Y_train.shape)  + "\n"
txtfile.write(printstr)

model_22=my_pretrained_model_2(new_model, max_len, 9)

with open('model_2_without_window.txt','w') as fh:
    model_22.summary(print_fn=lambda x: fh.write(x + '\n'))

load_file = "secondary_structure_vgg16_imagenet_pretrained_without_window_normal_phys.h5" # M: val_loss E: val_weighted_accuracy
checkpointer = ModelCheckpoint(filepath=load_file,verbose=1,save_best_only=True)

m2_history=model_22.fit({'main_input': phys_X_train}, {'main_output': phys_Y_train}, validation_data=({'main_input': phys_X_valid},{'main_output': phys_Y_valid}),
        epochs=100, batch_size=256,callbacks=[checkpointer], verbose=1, shuffle=True)
  
with open('model_2_without_window_normal_phys_history.json', 'w') as f:
    json.dump(m2_history.history, f)

model_22.load_weights(load_file)
print("#########evaluate:##############")
score = model_22.evaluate({'main_input': phys_X_test1},{'main_output': phys_Y_test1}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " without window normal phys test1 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_22.load_weights(load_file)
score = model_22.evaluate({'main_input': phys_X_test2},{'main_output': phys_Y_test2}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " without window normal phys test2 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_22.load_weights(load_file)
score = model_22.evaluate({'main_input': phys_X_test3},{'main_output': phys_Y_test3}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " without window normal phys test3 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

hot_X_test1,hot_Y_test1=seq_to_one_hot(raw_primer_test1,max_len),label_to_one_hot(raw_seconder_test1,max_len)
hot_X_test2,hot_Y_test2=seq_to_one_hot(raw_primer_test2,max_len),label_to_one_hot(raw_seconder_test2,max_len)
hot_X_test3,hot_Y_test3=seq_to_one_hot(raw_primer_test3,max_len),label_to_one_hot(raw_seconder_test3,max_len)
hot_X_valid,hot_Y_valid=seq_to_one_hot(raw_primer_valid,max_len),label_to_one_hot(raw_seconder_valid,max_len)
hot_X_train,hot_Y_train=seq_to_one_hot(raw_primer_train,max_len),label_to_one_hot(raw_seconder_train,max_len)

hot_X_test1.shape, hot_Y_test1.shape, hot_X_test2.shape, hot_Y_test2.shape, hot_X_test3.shape, hot_Y_test3.shape, hot_X_valid.shape, hot_Y_valid.shape ,hot_X_train.shape, hot_Y_train.shape
printstr="hot_X_test1.shape, hot_Y_test1.shape, hot_X_test2.shape, hot_Y_test2.shape, hot_X_test3.shape, hot_Y_test3.shape, hot_X_valid.shape, hot_Y_valid.shape ,hot_X_train.shape, hot_Y_train.shape is :" + str(hot_X_test1.shape) + str(hot_Y_test1.shape) + str(hot_X_test2.shape) + str(hot_Y_test2.shape) + str(hot_X_test3.shape) +  str(hot_Y_test3.shape) + str(hot_X_valid.shape) + str(hot_Y_valid.shape) + str(hot_X_train.shape) + str(hot_Y_train.shape)  + "\n"
txtfile.write(printstr)

model_2=my_pretrained_model_2(new_model, max_len, 21)

load_file = "secondary_structure_vgg16_imagenet_pretrained_without_window_one_hot.h5" # M: val_loss E: val_weighted_accuracy
checkpointer = ModelCheckpoint(filepath=load_file,verbose=1,save_best_only=True)

m2_history=model_2.fit({'main_input': hot_X_train}, {'main_output': hot_Y_train}, validation_data=({'main_input': hot_X_valid},{'main_output': hot_Y_valid}),
        epochs=100, batch_size=256,callbacks=[checkpointer], verbose=1, shuffle=True)

with open('model_2_without_window_hot_history.json', 'w') as f:
    json.dump(m2_history.history, f)

model_2.load_weights(load_file)
print("#########evaluate:##############")
score = model_2.evaluate({'main_input': hot_X_test1},{'main_output': hot_Y_test1}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])

printstr="#########evaluate:############## \n"+ " without window one hot test1 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_2.load_weights(load_file)
score = model_2.evaluate({'main_input': hot_X_test2},{'main_output': hot_Y_test2}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])


printstr="#########evaluate:############## \n"+ " without window one hot test2 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

model_2.load_weights(load_file)
score = model_2.evaluate({'main_input': hot_X_test3},{'main_output': hot_Y_test3}, verbose=1, batch_size=32)
print(score) 
print('test loss:', score[0])
print('test accuracy:', score[1])


printstr="#########evaluate:############## \n"+ " without window one hot test3 \n"+ "test loss: " + str(score[0]) + "\n" + "test accuracy: " + str(score[1]) + "\n"
txtfile.write(printstr)

txtfile.close()