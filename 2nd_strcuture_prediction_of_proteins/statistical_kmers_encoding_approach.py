# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iyFE4nN30YDMEV_F6BQFjOlx5KJCMxk7
"""

pip install 'h5py==2.10.0' --force-reinstall

pip install keras_self_attention

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PZC6Y9jRc1MO9-K_j9ElcfdZ2Dl6-dol
"""

import copy
import pandas as pd
import numpy as np
from sklearn.manifold import TSNE
from joblib import Parallel, delayed
from sklearn.model_selection import train_test_split
import copy
import random
import urllib.request
import io
import gzip
import csv
import numpy as np
from collections import Counter

# input: name of the file which is going to be read 
# this function get the input filename of protein sequence in the formats of .txt, .npy , .csv 
# output: the related amino acids sequences with their corresponding secondary structure labels in the format of lists 
def read_data(filename):
    raw_primer= []
    raw_sekunder = []
    idx=0
    j=0

    if filename=='RS126.data.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_sekunder.append(line.strip())
                else:
                    raw_primer.append(line.strip())
                    
    if filename=='pdb_full_train.txt' or filename=='pdb_full_test.txt' or filename=='train.txt' or filename=='blind.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_primer.append(line.strip())                  
                        
    if filename=='pdb_full_train_dssp.txt' or filename=='pdb_full_test_dssp.txt' or filename=='train_label.txt' or filename=='blind_label.txt':
        with open(filename, 'r') as f:
            for count, line in enumerate(f, start=1):
                if count % 2 == 0:
                    raw_sekunder.append(line.strip())

    if filename=='cb513.npy':
        f=np.load(filename, allow_pickle=True , encoding='latin1')
        raw_sekunder=f.item(0)["dssp"]
        raw_primer=f.item(0)["seq"]
        
    if filename=='casp12.npy' or filename=='casp13.npy' or filename=='cullpdb_test.npy' or filename=='cullpdb_train.npy':
        data=np.load(filename).item()
        name=data['name']
        seq=data['seq']
        pssm=data['pssm']
        dssp=data['dssp']
        hhm=data['hhm']
    
    if filename=='cb513.csv' or filename=='cb6133.csv' or filename=='cb6133filtered.csv':
      df=pd.read_csv(filename, sep=',')
      seqs=df[["input"]].to_numpy()
      labels=df[["expected"]].to_numpy()
      raw_primer,raw_seconder=[],[]
      for i in range(len(seqs)):
        raw_primer.append(seqs[i][0])
        raw_seconder.append(labels[i][0])
                                 
    if filename=='RS126.data.txt' or filename=='cb513.npy':
        return raw_primer,raw_sekunder
    if filename=='casp12.npy' or filename=='casp13.npy' or filename=='cullpdb_test.npy' or filename=='cullpdb_train.npy':
        return name,seq,pssm,dssp,hhm
    if filename=='pdb_full_train.txt' or filename=='pdb_full_test.txt' or filename=='train.txt' or filename=='blind.txt':
        return raw_primer
    if  filename=='pdb_full_train_dssp.txt' or filename=='pdb_full_test_dssp.txt' or filename=='train_label.txt'  or filename=='blind_label.txt':
        return raw_sekunder
    if filename=='cb513.csv' or filename=='cb6133.csv' or filename=='cb6133filtered.csv':
      return raw_primer, raw_seconder
        
#  this function is written for some input files like RS126 whose sequences are needed to be processed 
#  like their labels have 3 labels which are better to be converted to an 8-label state since it is more common among all other input files 
def preprocess(filename,train,test,flag):
  if filename=='RS126.txt':
      for i in range(len(test)):
          word=str(test[i])
          test[i]=word.replace("_", "C")
          word=str(test[i])
          test[i]=word.replace("-", "C")
          word=str(test[i])
          test[i]=word.replace("G", "C")
          word=str(test[i])
          test[i]=word.replace("I", "C")
          word=str(test[i])
          test[i]=word.replace("B", "C")
          word=str(test[i])
          test[i]=word.replace("T", "C")
          word=str(test[i])
          test[i]=word.replace("S", "C")
          
      for i in range(len(train)):
          word=str(train[i])
          w=str(test[i])
          l=[]
          l=[pos for pos, char in enumerate(word) if char == 'X']
          ll=[pos for pos, char in enumerate(word) if char == 'Z']
          lll=[pos for pos, char in enumerate(w) if char == '?']
          lst=l+ll+lll
          s=train[i]
          t=test[i]
          n=0
          if not flag:
            for j in lst:
                s= s[0 : j-n : ] + s[j-n + 1 : :]
                t= t[0 : j-n : ] + t[j-n + 1 : :]
                n+=1
          train[i]=s
          test[i]=t
      

  raw_primer_train,raw_seconder_train=equal_length(train,test)
  return raw_primer_train,raw_seconder_train




# check if the primary sequence of all proteins and their corresponding secondary sequences are equal in length
# print the total amino acids in the file + number of proteins and return their primary and secondary lists
def equal_length(raw_primer,raw_seconder):
    count_sekunder = 0
    count_primer = 0
    l=[]
    for i in range(len(raw_seconder)):
        len1 = len(raw_seconder[i])
        len2 = len(raw_primer[i])
        count_sekunder = count_sekunder + len1
        count_primer = count_primer + len2
        if(len1 != len2):
            count_primer-=len2
            count_sekunder-=len1
            l.append(i)
    for i in l:
        raw_primer.pop(i)
        raw_seconder.pop(i)

    print("count struktur sekunder : ",count_sekunder,len(raw_primer))
    print("count struktur primer : ",count_primer,len(raw_seconder))

    return raw_primer,raw_seconder


# input: raw_primer_train,raw_primer_valid,raw_primer_test
# this function finds the maximum length of proteins among all train, validation, and test files
# this function is only used when we are going to make all protein into an equal length by padding
# output: an integer amount of maximum protein length
def find_max_len_in_all_seq(r1,r2,r3):
    max_len=0
    for i in range(len(r1)):
      if len(r1[i])>max_len:
        max_len=len(r1[i])
    for i in range(len(r2)):
      if len(r2[i])>max_len:
        max_len=len(r2[i])
    for i in range(len(r3)):
      if len(r3[i])>max_len:
        max_len=len(r3[i])
    return max_len

# input: list of secondary structures sequences + maximum length to be padded
# this function encodes the secondary structures of sequences into one-hot vectors and padded labels are labeled (0,0,0,0,0,0,0,0)
# output: the corresponding encoded lists in the shape of (number of proteins, max_len, 8)
def label_to_one_hot(label,max_len):
  # dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7 ,'*':8}
  dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
  out_label=[]
  for i in range(len(label)):
    one_hot=np.zeros((max_len,8))
    for j in range(len(label[i])):
      if label[i][j] in list(dssp.keys()):
        one_hot.itemset((j,dssp[label[i][j]]),1)
    out_label.append(one_hot)
  return np.array(out_label)

# input: list of secondary structures sequences
# this function encodes the secondary structures of sequences into one-hot vectors without any padding
# output: the corresponding encoded lists in the shape of (number of proteins, 8)
def label_to_one_hot2(label):
  # dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7 ,'*':8}
  dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}
  out_label=[]
  for i in range(len(label)):
    one_hot=[]
    for j in range(len(label[i])):
      if label[i][j] in list(dssp.keys()):
        l=[0 for i in range(8)]
        l[dssp[label[i][j]]]=1
        one_hot.append(l)
    out_label.append(one_hot)
  return out_label

# input: list of primary sequences + maximum length to be padded
# this function encodes the primary sequences into one-hot vectors and padded amino acids are labeled (0,0,0,.....,0,0,0) (length:21)
# output: the corresponding encoded lists in the shape of (number of proteins, max_len, 21)
def seq_to_one_hot(seq,max_len):

  # residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
  out=[]
  for i in range(len(seq)):
    one_hot=np.zeros((max_len,21))
    for j in range(len(seq[i])):
      if seq[i][j] in list(residue_list.keys()):
        one_hot.itemset((j,residue_list[seq[i][j]]),1)
    out.append(one_hot)

  return np.array(out)

# input: list of primary sequences
# this function encodes the primary sequences into one-hot vectors without any padding
# output: the corresponding encoded lists in the shape of (number of proteins, 21)
def seq_to_one_hot2(seq):

  # residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
  out=[]
  for i in range(len(seq)):
    one_hot=[]
    for j in range(len(seq[i])):
      if seq[i][j] in list(residue_list.keys()):
        l=[0 for i in range(21)]
        l[residue_list[seq[i][j]]]=1
        one_hot.append(l)
    out.append(one_hot)
  return out


# input: list of primary sequences + maximum length to be padded + 2-dimensional array consisting of 8 float numbers showing the probability of this aa to be the related label among all 8 structures
# this function encodes the primary sequences into vectors of length 8 and  padded amino acids are labeled (0,0,0,0,0,0,0,0) 
# output: the corresponding encoded lists in the shape of (number of proteins,max_len, 8) 
def seq_to_statistical_8_properties(seq,max_len,prob_aa_with_avg):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y','X']
  out=[]
  for i in range(len(seq)):
    eight_vector=np.zeros((max_len,8))
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        for k in range(8):
          eight_vector.itemset((j,k),prob_aa_with_avg[seq[i][j]][k])
      else:
        for k in range(8):
          eight_vector.itemset((j,k),0)     
    out.append(eight_vector)

  return np.array(out)


# input: 2-dimensional list 
# concat the entity of all entities into one vector
def target(lis):
    Y = []
    for i in range(len(lis)):
        for j  in range(len(lis[i])):
            Y.append(lis[i][j])
    return Y


# input: window size + required sequence needed to be slided by a window + encoding size of the amino acids in the sequence list (e.g. onehot=21)
# this function slide a window into the whole sequences
# output: the lists of all window-size frames in the shape of (,window_size, encod_size)
def window_padding_data(size, sequence, encode_size):
    num = int(size/2)
    if encode_size==1:
      zeros='0'
    else:
      zeros = np.array([0.0 for i in range(encode_size)])
    for i in range(len(sequence)):
        for j in range(num):
                sequence[i].append(zeros)
                sequence[i].insert(0, zeros)
            
    X = []
    temp = []

    for k in range(len(sequence)):
        for l in range(len(sequence[k])-(size-1)):
            temp = sequence[k][l:l+size]
            X.append(temp)
            temp = []

    return X

# input: two strings 
# this function calculates the number of non-identical characters in two strings of the same length
# output: return the calculated integer number 
def hamming_distance(str1, str2):
    return sum(c1 != c2 for c1, c2 in zip(str1, str2))

# input: two kmers + length of kmers=size + how many differences between the kmers can be accepted
# output: return True if two kmers have the same middle amino acids and they are like each other in at least d amino acids else False
def are_euqal(k1,k2,size,d):
  num = int(size/2)
  if k1[num]==k2[num] and hamming_distance(k1,k2)<= d:
    return True
  return False



# input: list of primary sequences +  list of secondary sequences + 2-dimensional array consisting of 8 float numbers showing the probability of this aa to be the related label among all 8 structures
# this function encodes the primary sequences into lists without any padding
# output: the corresponding encoded lists in the shape of (number of proteins, size, 8) and (number of proteins, size, 21)
def seq_to_statistical_8_properties_window(raw_primer,raw_seconder,prob_aa_with_avg):
  size=19
  primer,label=seq_to_statistical_8_properties2(raw_primer,prob_aa_with_avg),label_to_one_hot2(raw_seconder)
  X=window_padding_data(size, primer,8)
  Y=target(label)
  return np.array(X),np.array(Y)


# input  list of primary sequences + 2-dimensional array consisting of 8 float numbers showing the probability of this aa to be the related label among all 8 structures
# this function encodes the primary sequences into lists without any padding
# output: the corresponding encoded lists in the shape of (number of proteins, , 8)
def seq_to_statistical_8_properties2(seq,prob_aa_with_avg):
  residue_list ={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20, '*':21}
  amino_acids_order=['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y','X']
  out=[]
  for i in range(len(seq)):
    eight_vec=[]
    for j in range(len(seq[i])):
      if seq[i][j] in amino_acids_order:
        l=prob_aa_with_avg[(seq[i][j])]
        eight_vec.append(l)
      else:
        l=[0 for m in range(8)]
        eight_vec.append(l)     
    out.append(eight_vec)

  return out

# read the data
# 100 random sequences of 'cb513.csv for the test , 50 random sequences of 'cb6133.csv' for validation, and 200 random sequences of 'cb6133filtered.csv' for train

max_len=0
raw_primer_trainn,raw_seconder_trainn = read_data('cb513.csv')
raw_primer_test1,raw_seconder_test1=preprocess('cb513.csv',raw_primer_trainn,raw_seconder_trainn,False)
raw_primer_test,raw_seconder_test=raw_primer_test1,raw_seconder_test1


raw_primer_trainn,raw_seconder_trainn = read_data('cb6133.csv')
raw_primer_valid1,raw_seconder_valid1=preprocess('cb6133.csv',raw_primer_trainn,raw_seconder_trainn,False)
raw_primer_valid,raw_seconder_valid=raw_primer_valid1,raw_seconder_valid1


raw_primer_trainn,raw_seconder_trainn = read_data('cb6133filtered.csv')
raw_primer_train,raw_seconder_train=preprocess('cb6133filtered.csv',raw_primer_trainn,raw_seconder_trainn,False)

train_idx=random.sample(range(0,len(raw_primer_train)), k=200)
test_idx=random.sample(range(0,len(raw_primer_test)), k=100)
valid_idx=random.sample(range(0,len(raw_primer_valid)), k=50)
raw_primer_train,raw_seconder_train=[raw_primer_train[train_idx[i]] for i in range(len(train_idx))],[raw_seconder_train[train_idx[i]] for i in range(len(train_idx))]
raw_primer_valid,raw_seconder_valid=[raw_primer_valid[valid_idx[i]] for i in range(len(valid_idx))],[raw_seconder_valid[valid_idx[i]] for i in range(len(valid_idx))]
raw_primer_test,raw_seconder_test=[raw_primer_test[test_idx[i]] for i in range(len(test_idx))],[raw_seconder_test[test_idx[i]] for i in range(len(test_idx))]

max_len=find_max_len_in_all_seq(raw_primer_train,raw_primer_valid,raw_primer_test)

max_len+=(3-max_len%3)
length_test, length_valid,length_train=[len(i) for i in raw_primer_test],[len(i) for i in raw_primer_valid],[len(i) for i in raw_primer_train]

# concat the label of all the amino acids of all proteins into one string named all_label

all_label=""
for i in range(len(raw_seconder_train)):
  all_label+=raw_seconder_train[i]

# i=size= kmers length
# first find the kmers of all protein with window_padding_data function
# put all the kmers into one list named list_i_primer_join
# then for each kmer in list_i_primer_join we create a list in similar_kmers_with_distance that show only those subsequent kmers which are_euqal with this specific kmers 
#  so for each kmer i there is a list of close kmers

i=9
size=i
list_raw_primer_train_character=[list(raw_primer_train[i]) for i in range(len(raw_primer_train))]
train_i_mers=window_padding_data(size,list_raw_primer_train_character,1)
list_i_primer_join=[ ''.join(train_i_mers[i]) for i in range(len(train_i_mers))]
similar_kmers_with_distance=[list() for i in range(len(list_i_primer_join))]
for ii in range(len(list_i_primer_join)):
  k1=list_i_primer_join[ii]
  for j in range(ii+1,len(list_i_primer_join)):
    k2=list_i_primer_join[j]
    if are_euqal(k1,k2,size,round(size/4)):
      similar_kmers_with_distance[ii].append(j)

# since there are some kmers that are in multiple lists and they are close to multiple kmers they need to be processed in which each kmer should be only in one list and be close to only one kmer
# so among those kmers with this property we choose those kmers which they have the minimum Hamming distance with the related kmer and delete the related kemr from all other lists and just keep it in the preferred kmer list which has the min ham distance

new_similar_kmers_with_distance=copy.deepcopy(similar_kmers_with_distance)
for i in range(len(similar_kmers_with_distance)):
  all_i=[]
  min_ham=size
  min_idx=-1
  all_i=[]
  find=False
  for j in range(0,i):
      if i in similar_kmers_with_distance[j]:
        # print("i is:",i," and j is: ", j)
        find=True
        all_i.append(j)
        score=hamming_distance(list_i_primer_join[i],list_i_primer_join[j])
        if min_ham > score:
          min_ham= score
          min_idx= j
  for k in range(len(all_i)):
    if all_i[k]!=min_idx:
      new_similar_kmers_with_distance[all_i[k]].remove(i)

# create a dictionary with keys= index of the kmers like in the lists of list_i_primer_join and values= another dict that keeps the list of close kmers with the i kmer, the middle amino acids of the kmer, the label of the middle amino acids form all_label
# for those kmers whose new_similar_kmers_with_distance[i] is not empty which means they are at least close to one kmer
# dont pay attention to all_kmers and all_group list :)))

similar_kmers_with_distance_dict=dict()
all_kmers=[]
all_group=[]
for i in range(len(new_similar_kmers_with_distance)):
  if len(new_similar_kmers_with_distance[i])!=0:
    similar_kmers_with_distance_dict[i]=dict()
    similar_kmers_with_distance_dict[i]["close_kmers"] = new_similar_kmers_with_distance[i]
    similar_kmers_with_distance_dict[i]["close_kmers"].append(i)
    for g in new_similar_kmers_with_distance[i]:
      if g!=i:
        all_kmers.append(g)
    if i not in all_kmers:
      all_kmers.append(i)
    similar_kmers_with_distance_dict[i]["AA"] = list_i_primer_join[i][int(size/2)]
    similar_kmers_with_distance_dict[i]["kmer"] = list_i_primer_join[i]
    similar_kmers_with_distance_dict[i]["label"] = all_label[i]
    all_group.append(i)

# add those kmers which their new_similar_kmers_with_distance[i] is empty

for i in range(len(list_i_primer_join)):
  if i not in all_kmers:
    similar_kmers_with_distance_dict[i]=dict()
    similar_kmers_with_distance_dict[i]["close_kmers"] = [i]
    similar_kmers_with_distance_dict[i]["AA"] = list_i_primer_join[i][int(size/2)]
    similar_kmers_with_distance_dict[i]["kmer"] = list_i_primer_join[i]
    similar_kmers_with_distance_dict[i]["label"] = all_label[i]
    all_kmers.append(i)
    all_group.append(i)

prob_kmers_with_distance_dict=dict()
dssp={'L':0,'B':1,'E':2,'G':3,'I':4,'H':5,'S':6,'T':7}

# for each of the keys (kmers) in similar_kmers_with_distance_dict we have their close kmers so we create a list of 8 numbers and count each one of the labels have appeared as a label of the middle amino acids of the close kmers list 
# and calcualte the average of this list by dividing it into a number of close kmers for this kmer

for k,v in similar_kmers_with_distance_dict.items():
  prob_kmers_with_distance_dict[k]=[0.0 for i in range(8)]
  count_state=[0 for i in range(8)]
  close_kmers = v["close_kmers"]
  middle_amino_acid = v["AA"] 
  fisrst_kmer_to_be_group = v["kmer"]
  label_of_the_1st_kmer = v["label"]
  for m in range(len(close_kmers)):
    count_state[dssp[all_label[close_kmers[m]]]]+=1
  for i in range(len(count_state)):
      prob_kmers_with_distance_dict[k][i]=count_state[i]/len(close_kmers)

AA_lst={'A':0, 'C':1, 'E':2, 'D':3, 'G':4, 'F':5, 'I':6, 'H':7, 'K':8, 'M':9, 'L':10, 'N':11, 'Q':12, 'P':13, 'S':14, 'R':15, 'T':16, 'W':17, 'V':18, 'Y':19, 'X':20}
prob_aa_with_avg=dict()

# prob_aa_with_avg is a dict with 21 keys showing the encoding vector for each aa

# for each aa we find all the indexes which their corresponding kmer have the same aa in the middle and then sum all these vector and get their average

for acids in list(AA_lst.keys()):
  prob_aa_with_avg[acids]=[0.0 for i in range(8)]
  cnt=0
  for k,v in similar_kmers_with_distance_dict.items():
    middle_amino_acid = v["AA"] 
    if middle_amino_acid == acids:
      cnt+=1
      prob=prob_kmers_with_distance_dict[k]
      for p in range(len(prob)):
        prob_aa_with_avg[acids][p] += prob[p]
  for pp in range(len(prob_aa_with_avg[acids])):
    prob_aa_with_avg[acids][pp]/=cnt

# the code below contains all potential networks which models can be trained and tested on (needed to be explained more in the meeting)

import keras
from keras.models import Sequential
from keras.models import Model
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU
import tensorflow as tf
from keras.layers import Concatenate , Flatten, AveragePooling1D,AveragePooling2D, TimeDistributed, Bidirectional, GRU, concatenate, Dropout,Conv2DTranspose,Lambda
from keras.layers import Input, Embedding, LSTM,CuDNNLSTM, Dense, Convolution2D, GRU, Reshape,MaxPooling2D,Convolution1D,BatchNormalization, Conv1D, MaxPooling1D,merge, CuDNNLSTM
from keras_self_attention import SeqSelfAttention
from keras.optimizers import Adam
from keras import regularizers,Sequential
from keras.regularizers import l2
from keras.callbacks import EarlyStopping ,ModelCheckpoint
import matplotlib.pyplot as plt
import numpy as np
import random as rn
import pandas as pd
import tensorflow as tf
np.random.seed(1)
rn.seed(1)

from keras import backend as K
print(keras.__version__)
print(tf.__version__)
tf.compat.v1.set_random_seed(0)
session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
K.set_session(sess)




def model_4(max_len1,encode_size1,encode_size2):
    XDinput = Input(shape=(max_len1, encode_size1),name='main_input1')
    XTinput = Input(shape=(max_len1, encode_size2), name='main_input2')
    l2value=0.001
    NUM_FILTERS=5
    rnn_hidden_size1 = 128
    rnn_hidden_size2 = 128*2
    rnn_hidden_size3 = 128*3
    
    encode_smiles = Convolution1D(NUM_FILTERS,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(XDinput)
    encode_smiles = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_smiles)
    encode_smiles = Convolution1D(NUM_FILTERS*2,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(encode_smiles)
    encode_smiles = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_smiles)
    encode_smiles = Convolution1D(NUM_FILTERS*3,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(encode_smiles)
    encode_smiles = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_smiles) 
    encode_smiles = Dropout(0.4)(encode_smiles)
   
    encode_protein = Bidirectional(LSTM(rnn_hidden_size1, return_sequences=True,recurrent_activation='relu',dropout=0.5,recurrent_dropout=0.5))(XTinput)
    encode_protein = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_protein)
    encode_protein =  Bidirectional(LSTM(rnn_hidden_size2, return_sequences=True,recurrent_activation='relu',dropout=0.5,recurrent_dropout=0.5))(encode_protein)
    encode_protein = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_protein)
    encode_protein =  Bidirectional(LSTM(rnn_hidden_size3, return_sequences=True,recurrent_activation='relu',dropout=0.5,recurrent_dropout=0.5))(encode_protein)
    encode_protein = AveragePooling1D(pool_size=5, strides=1, padding='same')(encode_protein) 
    encode_protein = Dropout(0.4)(encode_protein)
    encode_interaction = Concatenate(axis=-1)([encode_protein, encode_smiles])
    encode_interaction = Dropout(0.4)(encode_interaction)
    encode_interaction = Flatten()(encode_interaction)

    

    FC1 = Dense(1024, activation='relu')(encode_interaction)
    FC2 = Dropout(0.4)(FC1)
    FC2 = Dense(512, activation='relu')(FC2)
    FC2 = Dropout(0.4)(FC2)
    FC2 = Dense(128, activation='relu')(FC2)
    predictions = Dense(8, activation='softmax', name='main_output')(FC2)

    interactionModel = Model(inputs=[XDinput, XTinput], outputs=[predictions])
    adam = Adam(learning_rate=0.003)
    interactionModel.compile(optimizer=adam,  loss={'main_output': 'categorical_crossentropy'}, weighted_metrics=['accuracy']) 
    print(interactionModel.summary())

    return interactionModel



def deep_aclstm_with_one_input_model(max_len, dim):
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input') 
    concat = main_input
    
    conv1_features = Convolution1D(20,1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(concat)
    conv1_features = Reshape((max_len, 20, 1))(conv1_features)
    
    conv2_features = Convolution2D(10,kernel_size=3,strides=1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv1_features)
    
    conv2_features = Reshape((max_len,-1))(conv2_features)
    conv2_features = Dropout(0.5)(conv2_features)
    conv2_features = Dense(100, activation='relu')(conv2_features)

    lstm_f1 = LSTM(100,return_sequences=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)
    lstm_b1 = LSTM(100, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)
    # concat_lstm1 = Concatenate(axis=-1)([lstm_f1, lstm_b1])
    # lstm_f2 = LSTM(300, return_sequences=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_f1)
    # lstm_b2 = LSTM(300, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_b1)
    # concat_lstm12 = Concatenate(axis=-1)([lstm_f2, lstm_b2])
    # bid2=Bidirectional(LSTM(output_dim=300, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5))(lstm_b1)
    concat_features = Concatenate(axis=-1)([lstm_f1, lstm_b1,conv2_features])
    # concat_features = Concatenate(axis=-1)([bid2, conv2_features])

    # protein_features= Flatten()(concat_features)
    protein_features=concat_features
    concat_features = Dropout(0.4)(protein_features)
    protein_features = Dense(100,activation='relu')(protein_features)
    dnse=Dense(50,activation='relu')
    protein_features = TimeDistributed(dnse)(protein_features)
    dnse=Dense(25,activation='relu', kernel_regularizer=l2(0.001))
    protein_features = TimeDistributed(dnse)(protein_features)
    
    main_output = Dense(8, activation='softmax', name='main_output')(protein_features)
  
    deepaclstm = Model(inputs=[main_input], outputs=[main_output])
    adam = Adam(lr=0.003)
    deepaclstm.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'},  weighted_metrics=['accuracy'])
    deepaclstm.summary()  
    return deepaclstm

def deep_aclstm_with_one_input_model_window(max_len, dim):
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input') 
    concat = main_input
    
    conv1_features = Convolution1D(20,1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(concat)
    conv1_features = Reshape((max_len, 20, 1))(conv1_features)
    
    conv2_features = Convolution2D(10,kernel_size=3,strides=1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv1_features)
    
    conv2_features = Reshape((max_len,-1))(conv2_features)
    conv2_features = Dropout(0.5)(conv2_features)
    conv2_features = Dense(100, activation='relu')(conv2_features)

    lstm_f1 = LSTM(100,return_sequences=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)
    lstm_b1 = LSTM(100, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)
    # concat_lstm1 = Concatenate(axis=-1)([lstm_f1, lstm_b1])
    # lstm_f2 = LSTM(300, return_sequences=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_f1)
    # lstm_b2 = LSTM(300, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_b1)
    # concat_lstm12 = Concatenate(axis=-1)([lstm_f2, lstm_b2])
    # bid2=Bidirectional(LSTM(output_dim=300, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5))(lstm_b1)
    concat_features = Concatenate(axis=-1)([lstm_f1, lstm_b1,conv2_features])
    # concat_features = Concatenate(axis=-1)([bid2, conv2_features])

    # protein_features= Flatten()(concat_features)
    protein_features=concat_features
    concat_features = Dropout(0.4)(protein_features)
    protein_features = Dense(100,activation='relu')(protein_features)
    dnse=Dense(50,activation='relu')
    protein_features = TimeDistributed(dnse)(protein_features)
    dnse=Dense(25,activation='relu', kernel_regularizer=l2(0.001))
    protein_features = TimeDistributed(dnse)(protein_features)
    protein_features = Flatten()(protein_features)
    main_output = Dense(8, activation='softmax', name='main_output')(protein_features)
  
    deepaclstm = Model(inputs=[main_input], outputs=[main_output])
    adam = Adam(lr=0.003)
    deepaclstm.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'},  weighted_metrics=['accuracy'])
    deepaclstm.summary()  
    return deepaclstm



def orginal_deep_ACLSTM(max_len,dim1, dim2):
  # design the deepaclstm model
    main_input = Input(shape=(max_len,), dtype='float32', name='main_input1')
    x = Embedding(output_dim=dim2, input_dim=dim2, input_length=max_len)(main_input)
    auxiliary_input = Input(shape=(max_len,dim1), name='main_input2') 
    print(main_input.get_shape())
    print(auxiliary_input.get_shape())
    concat = Concatenate(axis=-1)([x, auxiliary_input])    

    conv1_features = Convolution1D(42,1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(concat)
    conv1_features = Reshape((max_len, 42, 1))(conv1_features)    

    conv2_features = Convolution2D(42,3,1,activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv1_features)
    conv2_features = Reshape((max_len,42*42))(conv2_features)
    conv2_features = Dropout(0.5)(conv2_features)
    conv2_features = Dense(400, activation='relu')(conv2_features)

    lstm_f1 = LSTM(output_dim=300,return_sequences=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)
    lstm_b1 = LSTM(output_dim=300, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(conv2_features)    

    lstm_f2 = LSTM(output_dim=300, return_sequences=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_f1)
    lstm_b2 = LSTM(output_dim=300, return_sequences=True, go_backwards=True,recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(lstm_b1)    
    concat_features= Concatenate(axis=-1)([lstm_f2, lstm_b2, conv2_features])   

    concat_features = Dropout(0.4)(concat_features)
    protein_features = Dense(600,activation='relu')(concat_features)
    dnse=Dense(300,activation='relu')
    protein_features = TimeDistributed(dnse)(protein_features)
    dnse=Dense(50,activation='relu', kernel_regularizer=l2(0.001))
    protein_features = TimeDistributed(dnse)(protein_features)    
    main_output = Dense(8, activation='softmax', name='main_output')(protein_features)  
    

    model = Model(input=[main_input, auxiliary_input], output=[main_output])
    adam = Adam(lr=0.003)
    model.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'}, metrics=['accuracy'], weighted_metrics=['accuracy'])
    model.summary()
    return model

def model_3_with_window(max_len,dim):
  main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input') 
  conv_hidden_size = 64
  rnn_hidden_size = 256
  conv1_features = Convolution1D(conv_hidden_size,3,padding='same',activation='relu')(main_input)
  conv2_features = Convolution1D(conv_hidden_size,7,padding='same', activation='relu')(main_input)
  conv3_features = Convolution1D(conv_hidden_size,11,padding='same',activation='relu')(main_input)
  concat_features= Concatenate(axis=-1)([conv1_features, conv2_features, conv3_features])
  # concat_features=tf.transpose(concat_features , perm=[1,2])
  ggru=Bidirectional(LSTM(rnn_hidden_size, return_sequences=True,recurrent_activation='relu',dropout=0.5,recurrent_dropout=0.5))(concat_features)
  concat_features= Concatenate(axis=-1)([ggru, concat_features]) 
  concat_features = Flatten()(concat_features)
  dnse=Dense(500, activation='relu')(concat_features)
  concat_features=Dropout(0.4)(concat_features)
  dnse=Dense(300, activation='relu')(concat_features)
  concat_features=Dropout(0.4)(concat_features)
  dnse=Dense(50, activation='relu')(concat_features)
  main_output=Dense(8, activation='softmax' , name='main_output')(dnse)
  model = Model(input=[main_input], output=[main_output])
  adam = Adam(lr=0.003)
  model.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'}, weighted_metrics=['accuracy'])
  model.summary()
  return model

def model_3_without_window(max_len,dim):
  main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input') 
  conv_hidden_size = 64
  rnn_hidden_size = 256
  conv1_features = Convolution1D(conv_hidden_size,3,padding='same',activation='relu')(main_input)
  conv2_features = Convolution1D(conv_hidden_size,7,padding='same', activation='relu')(main_input)
  conv3_features = Convolution1D(conv_hidden_size,11,padding='same',activation='relu')(main_input)
  concat_features= Concatenate(axis=-1)([conv1_features, conv2_features, conv3_features])
  # concat_features=tf.transpose(concat_features , perm=[1,2])
  ggru=Bidirectional(LSTM(rnn_hidden_size, return_sequences=True,recurrent_activation='relu',dropout=0.5,recurrent_dropout=0.5))(concat_features)
  concat_features= Concatenate(axis=-1)([ggru, concat_features]) 
  # concat_features = Flatten()(concat_features)
  dnse=Dense(500, activation='relu')(concat_features)
  concat_features=Dropout(0.4)(concat_features)
  dnse=Dense(300, activation='relu')(concat_features)
  concat_features=Dropout(0.4)(concat_features)
  dnse=Dense(50, activation='relu')(concat_features)
  main_output=Dense(8, activation='softmax' , name='main_output')(dnse)
  model = Model(input=[main_input], output=[main_output])
  adam = Adam(lr=0.003)
  model.compile(optimizer = adam, loss={'main_output': 'categorical_crossentropy'}, weighted_metrics=['accuracy'])
  model.summary()
  return model





def build_model_e(max_len, dim, convs=[3, 5, 7], dense_size=200, dropout_rate=0.5,
                features_to_use=['onehot', 'pssm'], filter_size=256, lr=0.001,
                use_CRF=False):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
            idx = str(idx + 1)
            conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
                Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                      kernel_regularizer=l2(0.001))(main_input)))
    conc = concatenate(conclayers)

    drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
    dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)


    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)
    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='dense'))(dense_convinpn)
        crf = ChainCRF(name="crf1")
        crf_output = crf(timedist)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = TimeDistributed(Dense(8, activation='softmax'), name="main_output")(dense_convinpn)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                      sample_weight_mode='temporal')

    print(model.summary())
    return model
  

def build_model_d(max_len,dim,convs=[3, 5, 7], dense_size=64, lstm_size=64, dropout_rate=0.5, features_to_use=['onehot', 'pssm'], filter_size=64, lr=0.001, use_CRF=False, attention_units=16, attention_type='additive'):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
            idx = str(idx + 1)
            conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
                Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                      kernel_regularizer=l2(0.001))(main_input)))
    conc = concatenate(conclayers)

    if dropout_rate > 0:
          drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)
    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    lstm_out = Dense(dense_size, activation='relu')(drop_after_lstm)


    # Attention layer
    seq_representation = SeqSelfAttention(units=attention_units, attention_type=attention_type,
                                          name='Attention')(lstm_out)
    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='timedist'))(seq_representation)
        crf = ChainCRF(name="crf1")
        main_output = crf(timedist)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=0.001)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = TimeDistributed(Dense(8, activation='softmax'), name="main_output")(seq_representation)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=0.001)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                      sample_weight_mode='temporal')
    print(model.summary())
    return model




def build_model_c(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, CRF_input_dim=200, lr=0.001):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
            idx = str(idx + 1)
            conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
                Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                      kernel_regularizer=l2(0.001))(main_input)))
    conc = concatenate(conclayers)

    if dropout_rate > 0:
          drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
          dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)
    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(CRF_input_dim, activation='relu')(drop_after_lstm)

    timedist = TimeDistributed(Dense(8, name='crf_in'))(dense_out)
    crf = ChainCRF(name="crf1")
    main_output = crf(timedist)
    model = Model(input=[main_input], output=[main_output])
    adam=optimizers.Adam(lr=lr)
    model.compile(loss=crf.loss, optimizer=adam, weighted_metrics= ['accuracy'], sample_weight_mode='temporal')
    print(model.summary())
    return model

def build_model_b(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001, use_CRF=False):
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)

    # Highway
    highway_layer = Dense(dense_size, activation='relu')(drop_after_lstm)
    highway_out = Dense(dense_size, activation='relu')(highway_layer)

    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='crf_in'))(highway_out)
        crf = ChainCRF(name="crf1")
        main_output = crf(timedist, name="main_output")
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = TimeDistributed(Dense(8, activation='softmax'), name="main_output")(highway_out)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'])
    print(model.summary())
    return model

def build_model_b_window(max_len,dim,convs=[3, 5, 7], dense_size=64, lstm_size=64, dropout_rate=0.5, filter_size=64, lr=0.001, use_CRF=False):
    l2value=0.001
    main_input = Input(shape=(max_len,dim), dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]

    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)

    # Highway
    
    highway_layer = Dense(dense_size, activation='relu')(drop_after_lstm)
    main_output= Flatten()(highway_layer)
    highway_out = Dense(dense_size, activation='relu')(main_output)

    if use_CRF:
        timedist = TimeDistributed(Dense(8, name='crf_in'))(highway_out)
        crf = ChainCRF(name="crf1")
        main_output = crf(timedist, name="main_output")
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss=crf.loss, optimizer=adam, weighted_metrics=['accuracy'], sample_weight_mode='temporal')
    else:
        main_output = Dense(50,activation='relu',kernel_regularizer=l2(l2value))(main_output)
        main_output = Dense(25,activation='relu',kernel_regularizer=l2(l2value))(main_output)
        main_output = Dense(20,activation='relu',kernel_regularizer=l2(l2value))(main_output)
        main_output = Dense(8,activation='relu',kernel_regularizer=l2(l2value), name='main_output')(main_output)
        model = Model(input=[main_input], output=[main_output])
        adam = Adam(lr=lr)
        model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'])
    print(model.summary())
    return model

def build_model_a(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001):
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]
    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(dense_size, activation='relu')(drop_after_lstm)

    # Labeling layer layer
    main_output = TimeDistributed(Dense(8, activation='softmax'), name='main_output')(dense_out)
    # model = Model(inputs=main_input, outputs=timedist)
    model = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=lr)
    model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'],
                  sample_weight_mode='temporal')


    # print model
    print(model.summary())
    return model
def build_model_a_window(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001):
    l2value=0.0001
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]
    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=regularizers.l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(dense_size, activation='relu')(drop_after_lstm)
    

    main_output= Flatten()(dense_out)
    main_output = Dense(1000,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(500,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(100,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(20,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(8,activation='relu',kernel_regularizer=l2(l2value), name='main_output')(main_output)
    model = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=lr)
    model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'])


    # print model
    print(model.summary())
    return model

def build_model_a_without_window(max_len,dim,convs=[3, 5, 7], dense_size=200, lstm_size=400, dropout_rate=0.5, filter_size=256, lr=0.001):
    l2value=0.0001
    main_input = Input(shape=(max_len,dim),dtype='float32', name='main_input')
    input = BatchNormalization(name='batchnorm_input')(main_input)
    conclayers = [input]
    for idx, conv in enumerate(convs):
        idx = str(idx + 1)
        conclayers.append(BatchNormalization(name='batch_norm_conv' + idx)(
            Convolution1D(filter_size, conv, activation="relu", padding="same", name='conv' + idx,
                   kernel_regularizer=regularizers.l2(0.001))(input)))
    conc = concatenate(conclayers)

    # Dropout and Dense Layer before LSTM
    if dropout_rate > 0:
        drop_before = Dropout(dropout_rate, name='dropoutonconvs')(conc)
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(drop_before)
    else:
        dense_convinp = Dense(dense_size, activation='relu', name='denseonconvs')(conc)

    # Batch normalize the results of dropout
    dense_convinpn = BatchNormalization(name='batch_norm_dense')(dense_convinp)

    # LSTM
    lstm = Bidirectional(CuDNNLSTM(lstm_size, return_sequences=True, name='bilstm'))(dense_convinpn)
    drop_after_lstm = Dropout(dropout_rate)(lstm)
    dense_out = Dense(dense_size, activation='relu')(drop_after_lstm)
    

    # main_output= Flatten()(dense_out)
    main_output = Dense(1000,activation='relu',kernel_regularizer=l2(l2value))(dense_out)
    main_output = Dense(500,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(100,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(20,activation='relu',kernel_regularizer=l2(l2value))(main_output)
    main_output = Dense(8,activation='relu',kernel_regularizer=l2(l2value), name='main_output')(main_output)
    model = Model(input=[main_input], output=[main_output])
    adam = Adam(lr=lr)
    model.compile(loss={'main_output': 'categorical_crossentropy'}, optimizer=adam, weighted_metrics=['accuracy'])


    # print model
    print(model.summary())
    return model

def two_DCNN_BGRU(max_len,dim1):

    l2value=0.001
    NUM_FILTERS=5
    rnn_hidden_size1 = 128
    rnn_hidden_size2 = 128*2
    rnn_hidden_size3 = 128*3

    main_input = Input(shape=(max_len,dim1), dtype='float32', name='main_input')
    # x1 = Embedding(output_dim=dim2, input_dim=dim2)(main_input)
    # auxiliary_input = Input(shape=(max_len,dim1), name='main_input2') 
    # x2 = Embedding(output_dim=dim1, input_dim=dim1)(auxiliary_input)


    # encode_smiles = Convolution2D(NUM_FILTERS,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(x1)
    # encode_smiles = AveragePooling2D(pool_size=5, strides=1, padding='same')(encode_smiles)
    # encode_smiles = Convolution2D(NUM_FILTERS*2,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(encode_smiles)
    # encode_smiles = AveragePooling2D(pool_size=5, strides=1, padding='same')(encode_smiles)
    # encode_smiles = Convolution2D(NUM_FILTERS*3,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(encode_smiles)
    # encode_smiles = AveragePooling2D(pool_size=5, strides=1, padding='same')(encode_smiles) 
    # encode_smiles = Dropout(0.4)(encode_smiles)

    # print("encode_smiles.get_shape()",encode_smiles.get_shape())

    c_input=main_input
    c_output = Convolution1D(30,3,activation='relu', padding='same', kernel_regularizer=l2(0.001))(c_input)
    # print('c_output', c_output.get_shape())

    m_output = MaxPooling1D(pool_size=5, strides=1, padding='same')(c_output)
    # print('1m_output.getshape()',m_output.get_shape())
    # m_output = Reshape((max_len,30*21))(m_output)
    m_output = Dropout(0.5)(m_output)
    # print('2m_output.getshape()',m_output.get_shape())
    m_output = Dense(400, activation='relu')(m_output)
    # print('3m_output.getshape()',m_output.get_shape())
    # print(m_output.shape[2],m_output.shape[3])
    # m_output = Reshape((m_output.shape[1],m_output.shape[2]*m_output.shape[3]))(m_output)
    # print('4m_output.getshape()',m_output.get_shape()) 

    ########BLSTM Recurrent Neural networks##########
    f1 = GRU(units=200,return_sequences=True, activation='tanh', recurrent_activation='sigmoid',dropout=0.5,recurrent_dropout=0.5)(m_output)
    # print("im heree after f1/")
    f2 = GRU(units=200, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', go_backwards=True,dropout=0.5,recurrent_dropout=0.5)(m_output)
    # print("im heree after f2")
    # f = merge([d,e], mode='sum',concat_axis=2)
    # f=Dropout(0.5)(f)
    f3 = GRU(units=200, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', dropout=0.5,recurrent_dropout=0.5)(f1)
    f4 = GRU(units=200, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', go_backwards=True,dropout=0.5,recurrent_dropout=0.5)(f2)
    cf_feature = keras.layers.concatenate([f3, f4 , m_output], axis=2)
    # print("im heree after cf_feature")
    cf_feature = Dropout(0.4)(cf_feature)
    f_input = Dense(600,activation='relu')(cf_feature)
    # print("f_input.get_shape()",f_input.get_shape())

    # encode_interaction = Concatenate(axis=-1)([f_input, encode_smiles])
    
    main_output = TimeDistributed(Dense(300))(f_input)
    # main_output = Reshape((-1,))(main_output)
    main_output = Dense(100,activation='relu')(main_output)
    main_output = Dense(20,activation='relu')(main_output)
    main_output = Dense(8,activation='softmax',name="main_output")(main_output)

    model = Model(inputs=[main_input], outputs=[main_output])
    adam = Adam(lr=0.003)
    model.compile(optimizer=adam,
                  loss={'main_output': 'categorical_crossentropy'},
                  # loss_weights={'main_output': 1},
                  weighted_metrics=['accuracy'])
    print(model.summary())
    return model


def main(train_1,train_2,train_label,val_1,val_2,val_label,test_1,test_2,test_label,max_len,dim1, dim2,str):

    # model = deep_aclstm_with_one_input_model(max_len,dim1)
    model = deep_aclstm_with_one_input_model_window(max_len,dim1)
    # model = build_model_b_window(max_len,dim1)
    # model = build_model_a_without_window(max_len,dim1)
    # model = build_model_a_window(max_len,dim1)
    # mode = model_3_without_window(max_len,dim1)
    # model = orginal_deep_ACLSTM(max_len,dim1,dim2)
    # model = new_model(max_len, dim1)
    # model = build_model_e(max_len=max_len, dim=dim1)
    # model = build_model_d(max_len=max_len, dim=dim1)
    # model = build_model_c(max_len=max_len, dim=dim1)
    # model = build_model_b(max_len=max_len, dim=dim1)
    # model = build_model_a_window(max_len=max_len, dim=dim1)
    # model = new_model_1(max_len,dim1,dim2)
    # model = two_DCNN_BGRU(max_len, dim1)
    
    # earlyStopping = EarlyStopping(monitor='val_weighted_accuracy', patience=5, verbose=1, mode='auto')
    earlyStopping = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='auto')
    load_file = str+".h5" # M: val_loss E: val_weighted_accuracy
    checkpointer = ModelCheckpoint(filepath=load_file,verbose=1,save_best_only=True)

    history=model.fit({'main_input': train_1}, {'main_output': train_label}, validation_data=({'main_input': val_1},{'main_output': val_label}),
            epochs=100, batch_size=64,callbacks=[checkpointer,earlyStopping], verbose=1, shuffle=True)

    # history=model.fit({'main_input1': train_1, 'main_input2': train_2}, {'main_output': train_label},validation_data=({'main_input1': val_1, 'main_input2': val_2},{'main_output': val_label}),
            # nb_epoch=45, batch_size=64, callbacks=[checkpointer,earlyStopping], verbose=1, shuffle=True)
    

    model.load_weights(load_file)
    print("#########evaluate:##############")
    score = model.evaluate({'main_input': test_1},{'main_output': test_label}, verbose=1, batch_size=32)
    # score = model.evaluate({'main_input1': test_1, 'main_input2': test_2},{'main_output': test_label}, verbose=1, batch_size=8)
    print(score) 
    print('test loss:', score[0])
    print('test accuracy:', score[1])

X_test_seq_to_statistical_8_properties,Y_test_seq_to_statistical_8_properties=seq_to_statistical_8_properties(raw_primer_test,max_len,prob_aa_with_avg),label_to_one_hot(raw_seconder_test,max_len)
X_valid_seq_to_statistical_8_properties,Y_valid_seq_to_statistical_8_properties=seq_to_statistical_8_properties(raw_primer_valid,max_len,prob_aa_with_avg),label_to_one_hot(raw_seconder_valid,max_len)
X_train_seq_to_statistical_8_properties,Y_train_seq_to_statistical_8_properties=seq_to_statistical_8_properties(raw_primer_train,max_len,prob_aa_with_avg),label_to_one_hot(raw_seconder_train,max_len)

X_test_seq_to_statistical_8_properties_window,Y_test_seq_to_statistical_8_properties_window=seq_to_statistical_8_properties_window(raw_primer_test,raw_seconder_test,prob_aa_with_avg)
X_valid_seq_to_statistical_8_properties_window,Y_valid_seq_to_statistical_8_properties_window=seq_to_statistical_8_properties_window(raw_primer_valid,raw_seconder_valid,prob_aa_with_avg)
X_train_seq_to_statistical_8_properties_window,Y_train_seq_to_statistical_8_properties_window=seq_to_statistical_8_properties_window(raw_primer_train,raw_seconder_train,prob_aa_with_avg)

X_test_seq_to_statistical_8_properties_window.shape,Y_test_seq_to_statistical_8_properties_window.shape,X_valid_seq_to_statistical_8_properties_window.shape,Y_valid_seq_to_statistical_8_properties_window.shape,X_train_seq_to_statistical_8_properties_window.shape,Y_train_seq_to_statistical_8_properties_window.shape

X_test_seq_to_statistical_8_properties.shape,Y_test_seq_to_statistical_8_properties.shape,X_valid_seq_to_statistical_8_properties.shape,Y_valid_seq_to_statistical_8_properties.shape,X_train_seq_to_statistical_8_properties.shape,Y_train_seq_to_statistical_8_properties.shape

main(X_train_seq_to_statistical_8_properties_window,None,Y_train_seq_to_statistical_8_properties_window,X_valid_seq_to_statistical_8_properties_window,None,Y_valid_seq_to_statistical_8_properties_window,X_test_seq_to_statistical_8_properties_window,None,Y_test_seq_to_statistical_8_properties_window,19,8,0,"model = build_model_b_window, encoding=seq_to_statistical_8_properties_window(train:500,val:200,test:100,epoch:100,batch:64,32,patience:10)")

main(X_train_seq_to_statistical_8_properties,None,Y_train_seq_to_statistical_8_properties,X_valid_seq_to_statistical_8_properties,None,Y_valid_seq_to_statistical_8_properties,X_test_seq_to_statistical_8_properties,None,Y_test_seq_to_statistical_8_properties,max_len,8,0,"model = build_model_b, encoding=seq_to_statistical_8_properties(train:500,val:200,test:100,epoch:100,batch:64,32,patience:10)")

