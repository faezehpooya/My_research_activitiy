# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AsOrZ70DGiSYE9EVEfhIuWrBEgdtw7WC
"""

pip install torch==1.6.0

import torch
torch.__version__

# !pip uninstall torch-scatter
# !pip uninstall torch-sparse
# !pip uninstall torch-geometric
#
# !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html
#
# !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cu102.html
#
# !pip install torch-geometric
#
# !pip uninstall torch-scatter
#
# !pip uninstall torch-sparse
#
# !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.6.0+cu102.html
#
# !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.6.0+cu102.html
#
#
# !pip install torch-geometric

import torch
import numpy as np
import json
import random
import math
import pickle
from warnings import warn
from datetime import datetime
import time
import os
import zipfile
from pprint import pprint
import tqdm
import torch
import torch.nn as nn
from torch.nn import AdaptiveAvgPool1d
import torch.nn.functional as F
from torch.nn import Sequential, Linear, ELU, AdaptiveMaxPool1d, L1Loss, MSELoss
from torch_geometric.nn.conv import NNConv, CGConv, GatedGraphConv, GraphConv,EGConv, SAGEConv, GravNetConv, GATv2Conv
from torch_geometric.nn import GCNConv, ChebConv, ARMAConv,GATConv, GatedGraphConv,GENConv,GMMConv,TAGConv,SGConv,APPNP,DenseGraphConv,DenseGCNConv, ResGatedGraphConv, TransformerConv,EdgePooling
from torch_geometric.nn import graclus, global_sort_pool, global_max_pool, global_add_pool, global_mean_pool, TopKPooling, SAGPooling, avg_pool_x, max_pool_x, EdgePooling
from torch_geometric.nn.pool import TopKPooling,ASAPooling

from torch_geometric.data import Data
from torch_geometric.data import DataLoader
from scipy import stats
import torch_geometric.nn as pyg_nn
import torch_geometric.utils as pyg_utils

import torch
import torch.nn as nn
import torch.distributed as dist
import torchvision
import torchvision.transforms as transforms

import argparse
from datetime import datetime
import os
import torch.multiprocessing as mp

class EarlyStopping:
    def __init__(self, patience: int):
        self.patience = patience
        self.min_delta = 0.001  # Minimum gain to be considered an improvement
        self.losses = []
        self.best_score = None
        self.counter = 0

    def check(self, loss):
        """Return bool whether we should stop the training or not"""
        self.losses.append(loss)

        if self.counter > self.patience:
            return True

        # First iteration
        if self.best_score is None:
            self.best_score = loss
        # Loss is better: reset the counter
        elif loss <= self.best_score - self.min_delta:
            self.best_score = loss
            self.counter = 0
        else:
            self.counter += 1
        
        return False


def compute_std_mean(samples, edges=True):
    def std_mean(index):
        population = torch.zeros(size=(len(samples[0][index]) * len(samples), len(samples[0][index][0])))
        for i, sample in enumerate(samples):
            population[i*len(sample[index]):i*len(sample[index])+len(sample[index]), :] = sample[index]

        return torch.mean(population, dim=0), torch.std(population, dim=0)

    mean_nodes, std_nodes = std_mean(index=0)
    if edges:
        mean_edges, std_edges = std_mean(index=2)
        return mean_nodes, std_nodes, mean_edges, std_edges
    else:
        return mean_nodes, std_nodes


def normalize(samples, ind, edges=True):
    all_samples = [samples[i] for i in ind]
    if edges:
        mean_nodes, std_nodes, mean_edges, std_edges = compute_std_mean(all_samples)

        return [
            (
                (sample[0] - mean_nodes) / std_nodes,
                sample[1],
                (sample[2] - mean_edges) / std_edges
            ) for i, sample in enumerate(samples)
        ]
    else:
        mean_nodes, std_nodes = compute_std_mean(all_samples, edges=False)
        return [
            (
                (sample[0] - mean_nodes) / std_nodes,
                sample[1]
            ) for i, sample in enumerate(samples)
        ]

def unify_information(path: str, bonds=False, rescale=False):
    # with open(path, "r") as f:
    #     read_file = json.load(f)
    read_file=json.loads(path)

    atoms = read_file["atoms"]
    for atom in atoms:
        atom["mass"] = read_file["atom_types"][atom["type"]]["mass"]
        atom["radius"] = read_file["atom_types"][atom["type"]]["radius"]

    edges = []
    _ = [[edges.append({
        "atoms": [i, j],
        "bonds": read_file["bonds"][i][j],
        "van_der_waals": read_file["van_der_waals"][i][j],
        "coulomb": read_file["coulomb"][i][j],
        "distance": read_file["distance"][i][j]
    }) for i, row in enumerate(read_file["bonds"])] for j, col in enumerate(read_file["bonds"])]

    angles = read_file["angles"]
    dihedrals = read_file["dihedrals"]

    return atoms, edges, angles, dihedrals



def get_graph(angles: list, atoms_list: list, angle_type: str, angles_list=None):
    nodes_tuple = set()
    for i, angle in enumerate(angles):
        first = sorted(angle["atoms"][:len(angle["atoms"]) - 1])
        second = sorted(angle["atoms"][1:])
        nodes_tuple.add(tuple(first))
        nodes_tuple.add(tuple(second))

    nodes = list(nodes_tuple)
  
    map_to_index = {k: v for v, k in enumerate(nodes)}
    # Each angle has two edges (since it's bidirectional)
    edge_index = np.zeros(shape=(2, 2 * len(angles)), dtype=np.long)
    edge_features = np.zeros(shape=(2 * len(angles), 1))
    for i, angle in enumerate(angles):
        # Sort is done in order to don't take order into account
        first = sorted(angle["atoms"][:len(angle["atoms"]) - 1])
        second = sorted(angle["atoms"][1:])
        edge_index[0, i] = map_to_index[tuple(first)]
        edge_index[1, i] = map_to_index[tuple(second)]
        edge_index[0, i + len(angles)] = map_to_index[tuple(second)]
        edge_index[1, i + len(angles)] = map_to_index[tuple(first)]
        value = float(angle["value"])
        edge_features[i, 0] = value
        edge_features[i + len(angles), 0] = value


    # map_to_index :{(i,j):0, (j,k):1, ......} i, j,k : index atoms_list
    # edge_index : index hash is map to index maid masaln 0:(i,j) 1:(j,k)
    # [
    # [36,  0, 12, 12, 33, 33, 33,....,16, 31,  0, 31, 12,  0, 31.....,]
    # [16, 31,  0, 31, 12,  0, 31,....,36,  0, 12, 12, 33, 33, 33,....,]
    # ]
    # edge_features : value angle 
    # [[116.2724],
    # [112.6077],
    # [108.9494],
    # [104.2379],
    # .....
    # ]


    # TODO: Understand if nodes in this graph have features or not. For now give them at least the sum of the masses
    nodes_features = np.zeros(shape=(len(nodes), len(nodes[0])*len(NODES_FEATURES['atoms'])))
    for i, node in enumerate(nodes):
        s=[]
        for atom in node:
           for prop in  NODES_FEATURES['atoms']:
             s.append(atoms_list[atom][prop])
        nodes_features[i] = s

  # node_features:
  #     masss     radious  pcharge
  # [[ 13.0180,   1.0130,  -4.6266],
  #  [ 13.0180,   1.0130,  -2.2268],
  #  [ 13.0180,   1.0130,  -2.2268],
  #  [ 15.0180,   0.6910,  -2.6204],
  #  [ 13.0180,   0.4950,   2.1138],
  # ....
  # ]
    
    return (
        torch.from_numpy(nodes_features).float(),
        torch.from_numpy(edge_index).long(),
        torch.from_numpy(edge_features).float()
    )



def get_graph_2(atoms, angles, dihedrals, shuffle=False, sin_cos_decomposition=False):
    atoms_to_dihedral = {
        tuple(sorted(dihedral["atoms"])): dihedral["value"] for dihedral in dihedrals
    }
    atoms_to_angles = {
        tuple(sorted(angle["atoms"])): angle["value"] for angle in angles
    }

    angle_dihedral_nodes=[]
    for dihedral in dihedrals:
        angle_dihedral_nodes.append(tuple(sorted(dihedral["atoms"])))
    for angle in angles:
        angle_dihedral_nodes.append(tuple(sorted(angle["atoms"])))
    

    edges = []
    nodes_tuple = set()
    for i, dihedral_angle1 in enumerate(angle_dihedral_nodes):
      for j, dihedral_angle2 in enumerate(angle_dihedral_nodes):
        if dihedral_angle2!=dihedral_angle1:
          atoms_in_common = [i for i in dihedral_angle1 if i in dihedral_angle2]
          if len(atoms_in_common) >= 1:
            nodes_tuple.add(tuple(dihedral_angle1))
            nodes_tuple.add(tuple(dihedral_angle2))
            # if [i,j] not in edges:
            edges.append([i, j])

    nodes = list(nodes_tuple)
    nodes_features = np.zeros(shape=(len(nodes), 1))
    for i, node in enumerate(nodes):
        if len(node)==3:
          value = atoms_to_angles[node]
        elif len(node)==4:
          value = atoms_to_dihedral[node]
        nodes_features[i] = value

    edge_index = np.asarray(edges).transpose()
    angle_dihedral_nodes = np.array(angle_dihedral_nodes)
    return torch.from_numpy(nodes_features, requires_grad=True).float(), torch.from_numpy(edge_index , requires_grad=True).long(), angle_dihedral_nodes

class Net_0(nn.Module):
    def __init__(self, sample, nodes1=256, nodes2=1024, nodes3=128, nodes4=32, layers=4):
        super(Net_0, self).__init__()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.nodes = len(sample.x)
        self.nodes_features = sample.num_node_features
        self.input = nn.Linear(self.nodes * self.nodes_features, nodes1) 
        self.middle = nn.Linear(nodes1, nodes2)
        self.middle2 = nn.Linear(nodes2, nodes3)
        self.middle3 = nn.Linear(nodes3, nodes4)
        self.output = nn.Linear(nodes4, 1) 

        self.layers = layers

    def forward(self, sample):
        x = sample.x.view(-1)

        x = self.input(x)
        x = F.relu(x)
        x = self.middle(x)
        x = F.gelu(x)
        x = self.middle2(x)
        x = F.relu(x)
        x = self.middle3(x)
        x = F.relu(x)

        return self.output(x)
  
class Net_1(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_7, self).__init__()
        self.nodes_features = sample.num_node_features
        self.conv1 = GATConv(self.nodes_features, node1)
        self.conv2 = GATConv(node1, node2)
        self.conv3 = GATConv(node2, node3)
        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            AdaptiveAvgPool1d(41),
            Flatten(),
            Flatten(),
            nn.Linear(41*node3 ,hidden_size_1),
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,sample):
        x, edge_index = sample.x, sample.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.dropout(x, training=self.training)
        x = self.conv3(x, edge_index)
        x = F.dropout(x, training=self.training)
        x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)

class Net_1_batch(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_7_batch, self).__init__()
        self.nodes_features = sample.num_node_features
        self.conv1 = GATConv(self.nodes_features, node1)
        self.conv2 = GATConv(node1, node2)
        self.conv3 = GATConv(node2, node3)
        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            Flatten(),
            nn.Linear(41*node3 ,hidden_size_1),
            nn.Dropout(0.25), 
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.dropout(x, training=self.training)
        x = self.conv3(x, edge_index)
        x = F.dropout(x, training=self.training)
        x = global_max_pool(x, batch)
        x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)


class Net_1_batch_2(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_7_batch_2, self).__init__()
        self.nodes_features = sample.num_node_features
        self.convs = nn.ModuleList()
        self.conv1 = GATConv(self.nodes_features, node1)
        self.conv2 = GATConv(node1, node2)
        self.conv3 = GATConv(node2, node3)
        self.conv4 = GATConv(node3, node3)
        self.convs.append(self.conv1)
        self.convs.append(self.conv2)
        self.convs.append(self.conv3)
        self.convs.append(self.conv4)
        self.post_mp = nn.Sequential(
            nn.Linear(node3, node4), nn.Dropout(0.25), 
            ELU(),
            nn.Linear(node4, 1),
            ELU())
        self.dropout = 0.25
        self.num_layers = 3

    def forward(self,data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        if data.num_node_features == 0:
          x = torch.ones(data.num_nodes, 1)

        for i in range(self.num_layers):
            x = self.convs[i](x, edge_index)
            emb = x
            x = F.relu(x) #
            x = F.dropout(x, p=self.dropout, training=self.training)
 
        x = pyg_nn.global_max_pool(x, batch)
        x = self.post_mp(x)

        return  torch.tanh(x)

class Net_2(torch.nn.Module):
    def __init__(self, sample, hidden_channels=64,hidden_channels1=32,hidden_channels2=16):
        super(Net_8, self).__init__()
        torch.manual_seed(12345)
        self.nodes_features = sample.num_node_features
        self.conv1 = ARMAConv(self.nodes_features, hidden_channels)
        self.conv2 = ARMAConv(hidden_channels, hidden_channels1)
        self.conv3 = ARMAConv(hidden_channels1, hidden_channels2)
        self.post_mp = nn.Sequential(
            nn.Linear(hidden_channels2, hidden_channels2), 
            nn.Dropout(0.25), 
            ELU(),
            nn.Linear(hidden_channels2, 1),
            ELU())

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        # 1. Obtain node embeddings 
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.conv3(x, edge_index)
        x = F.relu(x)


        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]

        x = F.dropout(x, p=0.5, training=self.training)
        x = self.post_mp(x)

        return  torch.tanh(x)

class Net_2_edge_weight(torch.nn.Module):
    def __init__(self, sample, hidden_channels=64,hidden_channels1=32,hidden_channels2=16):
        super(Net_8_edge_weight, self).__init__()
        torch.manual_seed(12345)
        self.nodes_features = sample.num_node_features
        self.conv1 = GCNConv(self.nodes_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels1)
        self.conv3 = GCNConv(hidden_channels1, hidden_channels2)
        self.post_mp = nn.Sequential(
            nn.Linear(hidden_channels2, hidden_channels2), 
            nn.Dropout(0.25), 
            ELU(),
            nn.Linear(hidden_channels2, 1),
            ELU())

    def forward(self, data):
        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_weight, data.batch
        # 1. Obtain node embeddings 
        x = self.conv1(x, edge_index,edge_weight)
        x = F.relu(x)
        x = self.conv2(x, edge_index,edge_weight)
        x = F.relu(x)
        x = self.conv3(x, edge_index,edge_weight)
        x = F.relu(x)


        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]

        x = F.dropout(x, p=0.5, training=self.training)
        x = self.post_mp(x)

        return  torch.tanh(x)



class Net_3(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_1, self).__init__()
        self.nodes_features = sample.num_node_features
        self.conv1 = GraphConv(self.nodes_features, node1)
        self.conv2 = GraphConv(node1, node2)
        self.conv3 = GraphConv(node2, node3)
        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            AdaptiveAvgPool1d(41),
            Flatten(),
            Flatten(),
            nn.Linear(41*node3 ,hidden_size_1),
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,sample):
        x, edge_index, edge_weight = sample.x, sample.edge_index, sample.edge_attr
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.dropout(x, training=self.training)
        x = self.conv3(x, edge_index)
        x = F.dropout(x, training=self.training)
        x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)

class Net_4(nn.Module):
    def __init__(self, data_sample, out_channels=24, iterations=4, hidden_output_nodes_1=260,hidden_output_nodes_2=16, output_nodes=1):
        super(Net_2, self).__init__()
        self.n_edge_features = data_sample.edge_attr.size()[1]
        self.n_node_features = data_sample.num_node_features

        self.iterations = iterations
        self.output_nodes = output_nodes
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.first_edge_nn = Sequential(
            Linear(self.n_edge_features, self.n_node_features * out_channels)
        ).to(device)
        self.edge_nn = Sequential(
            Linear(self.n_edge_features, out_channels * out_channels)
        ).to(device)
        self.nnconv_input = NNConv(data_sample.num_node_features, out_channels, self.first_edge_nn, aggr="add").to(device)
        self.pooling = TopKPooling(out_channels).to(device)
        self.nnconvs = [NNConv(out_channels, out_channels, self.edge_nn, aggr="add").to(device) for _ in range(iterations)]
        self.flatten = Flatten().to(device)
        self.output = Sequential(
            Linear(out_channels * data_sample.x.size()[0], hidden_output_nodes_1).to(device),
            ELU(),
            Linear(hidden_output_nodes_1, hidden_output_nodes_2).to(device),
            ELU(),
            Linear(hidden_output_nodes_2, output_nodes).to(device)
        ).to(device)

    def forward(self, sample):
        x, edge_index, edge_attr = sample.x, sample.edge_index, sample.edge_attr
        x = self.nnconv_input(x, edge_index, edge_attr)
        x = F.relu(x)

        for i in range(self.iterations):
            x = self.nnconvs[i](x, edge_index, edge_attr)
            x = F.relu(x)
        x = self.flatten(x)
        x = self.output(x)
        return torch.tanh(x)
  

class Net_5(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_3, self).__init__()
        self.nodes_features = sample.num_node_features
        self.conv1 = GMMConv(self.nodes_features, node1,1,100)
        self.conv2 = GMMConv(node1, node2,1,100)
        self.conv3 = GMMConv(node2, node3,1,100)
        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            AdaptiveAvgPool1d(41),
            Flatten(),
            Flatten(),
            nn.Linear(41*node3 ,hidden_size_1),
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,sample):
        x, edge_index, edge_weight = sample.x, sample.edge_index, sample.edge_attr
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.dropout(x, training=self.training)
        x = self.conv3(x, edge_index)
        x = F.dropout(x, training=self.training)
        x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)


class Net_5_v1(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_3_v1, self).__init__()
        self.nodes_features = sample.num_node_features
        self.gmm1 = GMMConv(self.nodes_features, node1,1,100)
        self.conv1 =  TransformerConv(node1, node2,edge_dim=1)
        self.gmm2 = GMMConv(node2, node3,1,100)
        self.conv2 =  TransformerConv(node3, node4,edge_dim=1)
        self.appnp1 = APPNP(K=2, alpha=0.5,dropout=0.2)

        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            AdaptiveAvgPool1d(41),
            Flatten(),
            Flatten(),
            nn.Linear(41*node4 ,hidden_size_1),
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,sample):
        x, edge_index, edge_weight = sample.x, sample.edge_index, sample.edge_attr
        x = F.relu(self.gmm1(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = F.relu(self.conv1(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = F.relu(self.gmm2(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = F.relu(self.conv2(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = self.appnp1(x, edge_index,edge_weight)
        x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)



class Net_6(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_4, self).__init__()
        self.nodes_features = sample.num_node_features
        self.conv1 = GMMConv(self.nodes_features, node1,1,100)
        self.conv2 = GMMConv(node1, node2,1,100)
        self.appnp1 = APPNP(self.nodes_features, node1)
        self.conv2 = APPNP(node1, node2)
        self.conv3 = APPNP(node2, node3)
        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            AdaptiveAvgPool1d(41),
            Flatten(),
            Flatten(),
            nn.Linear(41*node3 ,hidden_size_1),
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,sample):
        x, edge_index, edge_weight = sample.x, sample.edge_index, sample.edge_attr
        x = F.relu(self.conv1(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = F.relu(self.conv2(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x = F.relu(self.conv3(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)
        

class Net_7(torch.nn.Module):
    def __init__(self,sample,node1=128,node2=256,node3=64, node4=32):
        super(Net_5, self).__init__()
        self.nodes_features = sample.num_node_features
        self.n_edge_features = self.n_edge_features = sample.edge_attr.size()[1]
        self.first_edge_nn = Sequential(
        Linear(self.n_edge_features, self.nodes_features * node1)).to(device)
        self.conv1 = NNConv(self.nodes_features, node1, self.first_edge_nn, aggr="add").to(device)
        self.second_edge_nn = Sequential(
        Linear(self.nodes_features * node1, self.nodes_features * node1 * node2)).to(device)
        self.conv2 = NNConv(self.nodes_features* node1 * node2 , node2, self.second_edge_nn, aggr="add").to(device)
        
        hidden_size_1,hidden_size_2=500,100
        self.output = nn.Sequential(
            AdaptiveAvgPool1d(41),
            Flatten(),
            Flatten(),
            nn.Linear(41*node3 ,hidden_size_1),
            ELU(),
            nn.Linear(hidden_size_1,hidden_size_2),
            ELU(),
            Linear(hidden_size_2, 1)
        ).to(device)


    def forward(self,sample):
        x, edge_index, edge_weight = sample.x, sample.edge_index, sample.edge_attr
        x = F.relu(self.conv1(x, edge_index, edge_weight))
        x = F.dropout(x, training=self.training)
        print(x.shape, edge_index.shape,edge_weight.shape )
        x = self.conv2(x, edge_index, edge_weight)
        x = F.dropout(x, training=self.training)
        print(x.shape, edge_index.shape,edge_weight.shape )
        # x = self.conv3(x, edge_index, edge_weight)
        # x = F.dropout(x, training=self.training)
        # print(x.shape, edge_index.shape,edge_weight.shape )
        # x= self.output(x.reshape(x.size()[1], x.size()[0]).unsqueeze(dim=1))
        return torch.tanh(x)

class GNNStack(nn.Module):
    def __init__(self,sample,hidden_dim, output_dim, task='node'):
        super(GNNStack, self).__init__()
        self.task = task
        self.convs = nn.ModuleList()
        self.convs.append(self.build_conv_model(sample.num_node_features, hidden_dim))
        self.lns = nn.ModuleList()
        self.lns.append(nn.LayerNorm(hidden_dim))
        self.lns.append(nn.LayerNorm(hidden_dim))
        for l in range(2):
            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))

        # post-message-passing
        self.post_mp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), 
            nn.Linear(hidden_dim, output_dim))
        if not (self.task == 'node' or self.task == 'graph'):
            raise RuntimeError('Unknown task.')

        self.dropout = 0.25
        self.num_layers = 3

    def build_conv_model(self, input_dim, hidden_dim):
        # refer to pytorch geometric nn module for different implementation of GNNs.
        if self.task == 'node':
            return pyg_nn.GCNConv(input_dim, hidden_dim)
        else:
            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),
                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        if data.num_node_features == 0:
          x = torch.ones(data.num_nodes, 1)

        for i in range(self.num_layers):
            x = self.convs[i](x, edge_index)
            emb = x
            x = F.relu(x)
            x = F.dropout(x, p=self.dropout, training=self.training)
            if not i == self.num_layers - 1:
                x = self.lns[i](x)

        if self.task == 'graph':
            x = pyg_nn.global_max_pool(x, batch)

        x = self.post_mp(x)

        return  torch.tanh(x)



class Flatten(nn.Module):
    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, x):
        return x.view(-1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if device == "cpu":
    warn("You are using CPU instead of CUDA. The computation will be longer...")


seed = 923
random.seed(seed)
torch.manual_seed(seed)


TARGET_FILE = f"label_parsed.txt"
start=0
N_SAMPLES = 1500
NORMALIZE_DATA = True
NORMALIZE_TARGET = True
shuffle=True
train_split= 0.6
validation_split= 0.2
patience=50
epochs=50
learning_rate=0.001
#cyclic-LR

class RMSELoss(nn.Module):
    def __init__(self, eps=1e-6):
        super().__init__()
        self.mse = nn.MSELoss()
        self.eps = eps
        
    def forward(self,yhat,y):
        loss = torch.sqrt(self.mse(yhat,y) + self.eps)
        return loss

criterion = RMSELoss()
# L2-reguliztion 



indexes = [i for i in range(N_SAMPLES)]
random.shuffle(indexes)
indexes = indexes[:N_SAMPLES]
split = np.int(train_split*len(indexes))
train_ind = indexes[:split]
split_2 = split + np.int(validation_split*len(indexes))
validation_ind = indexes[split:split_2]
test_ind = indexes[split_2:]

archive = zipfile.ZipFile('pdb_1000.zip', 'r')

graph_samples = []
for i in range(start,start+N_SAMPLES):
        path=archive.read(str(i)+'.json').decode('utf-8')
        atoms, edges, angles, dihedrals = unify_information(path)
        graph=get_graph_2(atoms, angles, dihedrals)
        graph_samples.append(graph)


with open(TARGET_FILE, "r") as t:
    target = torch.as_tensor([torch.tensor([float(v)]) for v in t.readlines()][:N_SAMPLES])
    if not NORMALIZE_TARGET:
        target = target.reshape(shape=(len(target), 1))

# # Compute STD and MEAN only on training data
target_mean, target_std = 0, 1
if NORMALIZE_TARGET:
    target_std = torch.std(target, dim=0)
    target_mean = torch.mean(target, dim=0)
    target = ((target - target_mean) / target_std).reshape(shape=(len(target), 1))

if NORMALIZE_DATA:
    samples = normalize(graph_samples, train_ind, False)
else:
    samples = graph_samples

dataset = []
for i, sample in enumerate(samples):
    dataset.append(
        Data(x=sample[0], edge_index=sample[1], y=target[i]).to(device)
    )

data_size = len(dataset)
train_loader = DataLoader(dataset[:int(data_size * train_split)], batch_size=128, shuffle=True) # 512
val_loader = DataLoader(dataset[int(data_size * train_split):int(data_size * train_split)+int(data_size * validation_split)], batch_size=512, shuffle=True)
test_loader = DataLoader(dataset[int(data_size * train_split)+int(data_size * validation_split):], batch_size=512, shuffle=True)


for train_batch in train_loader:
  train_batch = train_batch.to(device)
for valid_batch in val_loader:
  valid_batch = valid_batch.to(device)
for test_batch in test_loader:
  test_batch = test_batch.to(device)

for data in train_loader:
    sample_batch= data
    break

# model = GNNStack(sample_batch,32, 1, 'graph').to(device)
model = Net_8(sample_batch.to(device))
stopping = EarlyStopping(patience=patience)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.8, weight_decay=1e-3)
scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=learning_rate, max_lr=0.01)

for param in model.conv1.parameters():
    param.requires_grad = True
for param in model.conv2.parameters():
    param.requires_grad = True
for param in model.conv3.parameters():
    param.requires_grad = True

# for name, param in model.state_dict().items():
#             print(name)
#             print("requires_grad: ", param.requires_grad)
#
# for p in model.named_parameters():
#             print("requires_grad: ", p[1].requires_grad)

#  with batch
from sklearn.metrics import accuracy_score
X_p=[]
y_p=[]
for i in range(epochs):
    model.train()
    t_start = time.time()
    occ=0
    dataiter = iter(train_loader)
    for batch in dataiter:
        optimizer.zero_grad()
        y_pred = model(batch) # to device
        print(y_pred.shape,  batch.y.shape)
        batch.y=batch.y.view(y_pred.shape[0],1)
        loss = criterion(y_pred, batch.y)
        print('loss is :', loss)
        loss.backward()
        optimizer.step()
        scheduler.step()

    # Compute validation loss
    model.eval()
    val_losses = []
    # save some memory
    with torch.no_grad():
        for v_batch in val_loader:
            y_pred = model(v_batch)
            print(y_pred.shape,  v_batch.y.shape)
            v_batch.y=v_batch.y.view(y_pred.shape[0],1)
            val_loss = criterion(y_pred, v_batch.y)
            val_losses.append(val_loss.item())

        val_loss = torch.mean(torch.as_tensor(val_losses)).item()
        if NORMALIZE_TARGET:
            val_loss = val_loss*target_std
        print("Epoch {} - Validation RMSE: {:.6f}".format(i+1, val_loss))
        X_p.append(i+1)
        y_p.append(val_loss)
        duration = (time.time() - t_start) / 60.0  # Minutes
        hours = np.int(np.floor(duration / 60.0))
        minutes = np.int(np.floor(duration - hours*60))
        print("duration : ", duration, " hours: ", hours, " minutes: ", minutes)
        if stopping.check(val_loss):
            epochs = i+1
            print(f"Training finished because of early stopping. Best loss on validation: {stopping.best_score}")
            break

import matplotlib.pyplot as plt
plt.plot(X_p, y_p)
plt.show()

predictions = []
targets=[]
errors = []
model.eval()
for data in test_loader:
  with torch.no_grad():
      # Forward pass: Compute predicted y by passing x to the model
      prediction = model(data.to(device))
      for p in prediction:
        predictions.append(p[0].item())
      for t in data.y:
        targets.append(t.item())

for i in range(len(predictions)):
  errors.append(predictions[i]-targets[i])

# Compute MAE
mae = np.absolute(np.asarray(errors)).mean()
if NORMALIZE_TARGET:
    mae *= target_std
print("Root mean square Error: {:.6f}".format(mae))
cr, pval = stats.pearsonr(targets, predictions)
print(cr)
print(pval)

Root mean square Error 8.329144-0.05366436103010801,0.35429981291973595

from sklearn.metrics import accuracy_score

for i in range(epochs):
    model.train()
    random.shuffle(train_ind)
    for number, j in enumerate(tqdm.tqdm(train_ind)):
        # Forward pass: Compute predicted y by passing x to the model
        y_pred = model(dataset[j].to(device))

        # Compute and print loss
        loss = criterion(y_pred, dataset[j].y)

        # Zero gradients, perform a backward pass, and update the weights.
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Compute validation loss
    model.eval()
    val_losses = []
    # save some memory
    with torch.no_grad():
        for j in validation_ind:
            y_pred = model(dataset[j].to(device))
            val_loss = criterion(y_pred, dataset[j].y)
            val_losses.append(val_loss.item())

        val_loss = torch.mean(torch.as_tensor(val_losses)).item()
        if NORMALIZE_TARGET:
            val_loss = val_loss*target_std
        print("Epoch {} - Validation RMSE: {:.6f}".format(i+1, val_loss))

        # Check Early Stopping
        if stopping.check(val_loss):
            epochs = i+1
            print(f"Training finished because of early stopping. Best loss on validation: {stopping.best_score}")
            break

duration = (time.time() - start) / 60.0  # Minutes
hours = np.int(np.floor(duration / 60.0))
minutes = np.int(np.floor(duration - hours*60))

predictions = []
targets=[]
errors = []
model.eval()
for j in test_ind:
    # Forward pass: Compute predicted y by passing x to the model
    prediction = model(dataset[j].to(device))
    error = prediction - dataset[j].y
    predictions.append(prediction.item())
    targets.append(dataset[j].y.item())
    errors.append(error.item())

# Compute MAE
mae = np.absolute(np.asarray(errors)).mean()
if NORMALIZE_TARGET:
    mae *= target_std
print("Root mean square Error: {:.6f}".format(mae))
cr, pval = stats.pearsonr(targets, predictions)
print(cr)
print(pval)

